{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing charter books\n",
    "\n",
    "The charter books of Holland and Zeeland have been scanned and OCR'ed and made available at the page level. This Jupyter Notebook contains code to:\n",
    "\n",
    "- make the text available at the charter level, such that users can jump to specific charters in stead of specific pages,\n",
    "- identify which places are attested in a charter,\n",
    "- identify in which year or period a charter was written,\n",
    "\n",
    "### Exploitable structure\n",
    "\n",
    "- pages have headers with charter number and year or year and charter number\n",
    "- charters are numbered in ascending order\n",
    "- charters have dates in ascending order\n",
    "- line numbers switch sides on alternating pages\n",
    "\n",
    "### Issues of preprocessing\n",
    "\n",
    "- lines are not properly split\n",
    "- several words are recognised as lists of individual characters separated by whitespace\n",
    "- several words are split over lines with first page followed by linebreak\n",
    "- margins contain line numbers \n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- identify charter titles\n",
    "- identify charter years\n",
    "- identify place names mentioned in charters\n",
    "- identify language of charter text paragraphs (Latin, Middle Dutch, Modern Dutch, French, ...)\n",
    "\n",
    "### Decisions\n",
    "\n",
    "Initially, the OCR'ed text on the [resources.huygens.knaw.nl](http://resources.huygens.knaw.nl/retroboeken/ohz/#page=0&accessor=toc&view=homePane) website was used, but it has issues with whitespacing within words (i.e. many words are split into whitespaced individual characters), which is difficult to resolve for sequences of words, e.g. \"l o c u m E p t e r n a c u m\" instead of \"locum Epternacum\" (OHZ part 1, page 5).\n",
    "\n",
    "Instead, the HTML formatted OCR output of a new OCR using Tesseract was used. Besides solving the issue with whitespacing, it seems to have slightly better recognition in that numbers are more often recognized correctly, which is important for identifying charter numbers and dates. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "import parse_hocr_files # local module\n",
    "from fuzzy_matcher import FuzzyMatcher # local module\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Charter Page Structure from hOCR files\n",
    "\n",
    "Read charter books per page and extract:\n",
    "\n",
    "- page headers, footers, line numbers, paragraphs\n",
    "- charter number, charter title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconfuse(text):\n",
    "    # Quick and dirty deconfusion for charter numbers and dates\n",
    "    text = re.sub(\"i\", \"1\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"z\", \"2\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"g\", \"9\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"o\", \"0\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"u(\\d+)\", r\"11\\1\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def prune_paragraph(paragraph):\n",
    "    # Efficiency stuff: prune paragraphs with the wrong characteristics\n",
    "    # 1. Charter titles can have multiple lines, but no more than 3\n",
    "    if len(paragraph[\"line_texts\"]) > 3:\n",
    "        return True\n",
    "    # 2. Charter titles never have full text lines with over 60 characters\n",
    "    most_chars_per_line = max([len(line.replace(\" \",\"\")) for line in paragraph[\"line_texts\"]])\n",
    "    if most_chars_per_line > 60:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def determine_lookup_number(next_number):\n",
    "    # Map charters with known OCR issues to OCR'ed representation.\n",
    "    # Some charter numbers are recognized as numbers with fewer digits.\n",
    "    # Easiest solution is to just list them and map to the recognized number\n",
    "    if next_number in known_ocr_errors: \n",
    "        return known_ocr_errors[next_number]\n",
    "    else:\n",
    "        return next_number\n",
    "\n",
    "def check_paragraph(paragraph, next_number):\n",
    "    # Check if this paragraph is a charter title. If so, extract charter number.\n",
    "    lookup_number = determine_lookup_number(next_number)\n",
    "    candidate = False\n",
    "    if prune_paragraph(paragraph):\n",
    "        return False\n",
    "    for line in paragraph[\"line_texts\"]:\n",
    "        # Generally: \n",
    "        # Charter numbers are almost always at least 20 characters/whitespaces from the start.\n",
    "        # Some special rules:\n",
    "        # Numbers above 2000 probably don't refer to a year,\n",
    "        # so if line contains right number, it's probably the charter number\n",
    "        if lookup_number > 2000 and re.search(r\".{20,}  %s \" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # Exception:\n",
    "        # Charters 2535 and 2536 are on a single line, 2535 is left-column-centred so close to left edge\n",
    "        # Only three whitespaces between date and number\n",
    "        if lookup_number > 2000 and re.search(r\".{10,} {3,}%s {4,}\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # Another exception:\n",
    "        # Two charter numbers separated by whitespace, e.g. charters 3075 and 3076\n",
    "        if re.search(r\".{20,} %s %s {2,}\" % (lookup_number-1, lookup_number), line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # Another exception:\n",
    "        # Two titles with dates, where first title number occurs early, centered above left column,\n",
    "        # so less than 20 characters after start of line, but with plenty of white space around it.\n",
    "        # E.g. charters 82 and 83 on page 150 in book 1, charters 849 and 850 on page 189 in book 2.\n",
    "        if re.search(r\".{10,} {4,}%s {4,}\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # Generally, if the expected number appears surrounded by several whitespaces, \n",
    "        # at least 20 characters from the start, it should be the charter number\n",
    "        if re.search(r\".{20,} {2,}%s {2,}\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # If the expected number is at the line (e.g. no geographical indication at the right side)\n",
    "        if re.search(r\".{20,} {2,}%s$\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # There are some exceptions where a range of charters are grouped, without\n",
    "        # spelling out each charter number, e.g. 2599-2608 in part 5, page 50.\n",
    "        # Solution: capture start and end number and returns as a tuple\n",
    "        if re.search(r\".{20,} {2,}%s-\\d+\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            m = re.search(r\" {2,}%s-(\\d+)\" % lookup_number, line)\n",
    "            print(\"MATCH:\", m.group(1))\n",
    "            sequence_end = int(m.group(1))\n",
    "            return (candidate, sequence_end)\n",
    "        # There is at least one charter title with three charter numbers separated by hyphens.\n",
    "        # The next two checks cover this exception.\n",
    "        if re.search(r\".{20,} {2,}(\\d+-)+%s\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        if re.search(r\".{20,} {2,}\\d+-%s-\\d+\" % lookup_number, line):\n",
    "            candidate = next_number\n",
    "            return candidate\n",
    "        # If above exceptions don't hold, look for the charter number at the centre of the string.\n",
    "        center_string = line[34:48]\n",
    "        if len(center_string) == 0: # if it's emtpy if doesn't have the charter number.\n",
    "            continue\n",
    "        # Determine how many text characters the charter number should take up\n",
    "        max_center_length = len(str(lookup_number)) + 1\n",
    "        if len(center_string.replace(\" \",\"\")) > max_center_length: \n",
    "            # if it takes up at least two more than characters than expected,\n",
    "            # it's not the charter number.\n",
    "            continue\n",
    "        center_text = center_string.strip()\n",
    "        center_text = deconfuse(center_text)\n",
    "        if center_text.isdigit():\n",
    "            # If the center text is a number, it should be the expected number\n",
    "            candidate = int(center_text)\n",
    "            return candidate\n",
    "    return candidate\n",
    "\n",
    "def has_charter_title(paragraph, numbers):\n",
    "    is_sequence = False\n",
    "    # The titles of some charters are missing because of missing pages in the hOCR output.\n",
    "    # If the title for a charter number is known to be missing, skip to the next charter number\n",
    "    if numbers[\"next_charter\"] in missing_numbers:\n",
    "        numbers[\"current_charter\"] = [numbers[\"next_charter\"]]\n",
    "        numbers[\"next_charter\"] += 1\n",
    "    # check if paragraph has a candidate title number\n",
    "    candidate = check_paragraph(paragraph, numbers[\"next_charter\"])\n",
    "    # candidate is normally an integer for a single charter number. If it's a tuple, the charter title\n",
    "    # contains a range of charter numbers\n",
    "    if isinstance(candidate, tuple):\n",
    "        is_sequence = True\n",
    "        sequence_end = candidate[1]\n",
    "        sequence_start = candidate[0]\n",
    "        candidate = candidate[0]\n",
    "    # Some charter numbers are missing a number in the OCR, making it hard to match. Use mapping table to \n",
    "    # determine what number in the OCR the look for\n",
    "    if numbers[\"next_charter\"] in known_ocr_errors and candidate == known_ocr_errors[numbers[\"next_charter\"]]:\n",
    "        candidate = numbers[\"next_charter\"]\n",
    "    while candidate == numbers[\"next_charter\"]:\n",
    "        paragraph[\"type\"] = \"charter_title\"\n",
    "        numbers[\"current_charter\"] = [numbers[\"next_charter\"]]\n",
    "        if \"charter_number\" not in paragraph:\n",
    "            paragraph[\"charter_number\"] = []\n",
    "        if is_sequence:\n",
    "            for candidate in range(sequence_start, sequence_end+1):\n",
    "                paragraph[\"charter_number\"] += [numbers[\"next_charter\"]]\n",
    "                print(\"CANDIDATE\", candidate, \"for charter\", numbers[\"next_charter\"], \"on page\", numbers[\"page_num\"], paragraph)\n",
    "                print(\"\\tFOUND:\", candidate)\n",
    "                numbers[\"next_charter\"] += 1\n",
    "                is_sequence = False\n",
    "        else:\n",
    "            paragraph[\"charter_number\"] += [numbers[\"next_charter\"]]\n",
    "            print(\"CANDIDATE\", candidate, \"for charter\", numbers[\"next_charter\"], \"on page\", numbers[\"page_num\"], paragraph)\n",
    "            print(\"\\tFOUND:\", candidate)\n",
    "            numbers[\"next_charter\"] += 1\n",
    "        while numbers[\"next_charter\"] in missing_numbers:\n",
    "            numbers[\"next_charter\"] += 1\n",
    "        candidate = check_paragraph(paragraph, numbers[\"next_charter\"])\n",
    "        if isinstance(candidate, tuple):\n",
    "            is_sequence = True\n",
    "            sequence_end = candidate[1]\n",
    "            sequence_start = candidate[0]\n",
    "            candidate = candidate[0]\n",
    "\n",
    "def process_charter_page(hocr_page, numbers):\n",
    "    for index, paragraph in enumerate(hocr_page.paragraphs):\n",
    "        is_charter_title = False\n",
    "        if numbers[\"page_num\"] > 1 and index == 0: # skip page headers, except on page 1 which has no header\n",
    "            paragraph[\"type\"] = \"header\"\n",
    "        elif index == len(hocr_page.paragraphs) - 1: # skip page footer which contains only page number\n",
    "            paragraph[\"type\"] = \"footer\"\n",
    "        else:\n",
    "            # Keep track of paragraph type, e.g. main_text, charter_title, header or footer\n",
    "            # By default, paragraph is main_text\n",
    "            paragraph[\"type\"] = \"main_text\"\n",
    "            # Next, check if paragraph is a charter title with the expected charter number\n",
    "            has_charter_title(paragraph, numbers)\n",
    "            # If so, change it's type\n",
    "            if paragraph[\"type\"] == \"charter_title\":\n",
    "                # keep track of current charter number(s)\n",
    "                numbers[\"current_charter\"] = paragraph[\"charter_number\"]\n",
    "            # If not, make sure subsequent paragraphs have the same charter number\n",
    "            # as the current charter title\n",
    "            if paragraph[\"type\"] == \"main_text\":\n",
    "                # pass on charter number(s) from title paragraph to later paragraphs of same charter\n",
    "                paragraph[\"charter_number\"] = numbers[\"current_charter\"]\n",
    "\n",
    "def index_page(hocr_page, book_num):\n",
    "    doc = {\n",
    "        \"book_num\": book_num,\n",
    "        \"page_num\": hocr_page.page_num,\n",
    "        \"lines\": hocr_page.lines,\n",
    "        \"paragraphs\": hocr_page.paragraphs,\n",
    "    }\n",
    "    doc_id = \"OHZ-{b}-page-{p}\".format(b=book_num, p=hocr_page.page_num)\n",
    "    es.index(index=\"retroboeken\", doc_type=\"ohz-page\", id=doc_id, body=doc)\n",
    "    \n",
    "def index_paragraphs(hocr_page, book_num):\n",
    "    for paragraph in hocr_page.paragraphs:\n",
    "        doc_id = \"OHZ-{b}-page-{page}-paragraph-{par}\".format(b=book_num, page=hocr_page.page_num, par=paragraph[\"paragraph_num\"])\n",
    "        paragraph[\"book_num\"] = book_num\n",
    "        paragraph[\"doc_id\"] = doc_id\n",
    "        es.index(index=\"retroboeken\", doc_type=\"ohz-paragraph\", id=doc_id, body=paragraph)\n",
    "    \n",
    "def get_hocr_files(hocr_dir):\n",
    "    for root, dirs, files in os.walk(hocr_dir):\n",
    "        for fname in sorted(files):\n",
    "            filepath = os.path.join(root, fname)\n",
    "            parts = fname.replace(\".hocr\",\"\").split(\"_\")\n",
    "            try:\n",
    "                page_num = int(parts[-1])\n",
    "                yield filepath, page_num\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "def process_book(book_num, first_charter_num):\n",
    "    numbers = {\n",
    "        \"current_charter\": [0],\n",
    "        \"next_charter\": first_charter_num\n",
    "    }\n",
    "    next_number = first_charter_num\n",
    "    curr_number = [0]\n",
    "    book_name = \"OHZ\" + str(book_num)\n",
    "    hocr_dir = \"hOCR/\" + book_name\n",
    "    page_index = {}\n",
    "    place_name_index = {}\n",
    "    non_places = {}\n",
    "    fuzzy_places = {}\n",
    "    print(\"Processing book {name} starting with charter number {number}\".format(name=hocr_dir, number=first_charter_num))\n",
    "    for filepath, page_num in get_hocr_files(hocr_dir):\n",
    "        numbers[\"page_num\"] = page_num\n",
    "        hocr_page = parse_hocr_files.make_hocr_page(filepath, page_num, remove_line_numbers=True)\n",
    "        process_charter_page(hocr_page, numbers)\n",
    "        #process_place_names(hocr_page, non_places)\n",
    "        #index_paragraphs(hocr_page, book_num)\n",
    "        if page_num > last_page[book_name]:\n",
    "            # Skip pages beyond the last body matter page\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index hOCR Pages of Charter Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if es.indices.exists(index='retroboeken'):\n",
    "#    es.indices.delete(index='retroboeken')\n",
    "\n",
    "next_number = 2\n",
    "for book_num in range(1,2):\n",
    "    book_name = \"OHZ{b}\".format(b=book_num)\n",
    "    process_book(book_num, starting_charter_numbers[book_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Track of OHZ Data Issues\n",
    "\n",
    "Things to keep track of:\n",
    "\n",
    "- starting charter number of each book\n",
    "- last page of main body matter\n",
    "- missing pages in hOCR output\n",
    "- missing charter titles because of missing pages\n",
    "- hard to interpret charter numbers because of OCR recognizing fewer characters than are printed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of the first charter in each book.\n",
    "starting_charter_numbers = {\n",
    "    \"OHZ1\": 1,\n",
    "    \"OHZ2\": 424,\n",
    "    \"OHZ3\": 1085,\n",
    "    \"OHZ4\": 1826,\n",
    "    \"OHZ5\": 2574,\n",
    "}\n",
    "\n",
    "# Identify which page number of the last page containing charter text.\n",
    "# Later pages contain literature references and other non-charter material.\n",
    "last_page = {\n",
    "    \"OHZ1\": 608,\n",
    "    \"OHZ2\": 776,\n",
    "    \"OHZ3\": 961,\n",
    "    \"OHZ4\": 989,\n",
    "    \"OHZ5\": 1137,\n",
    "}\n",
    "\n",
    "# The following charter numbers have been incorrectly OCR'ed. \n",
    "# When looking for charter titles, use the OCR'ed number.\n",
    "known_ocr_errors = {\n",
    "    1111: 111,\n",
    "    1112: 112,\n",
    "    1113: 113,\n",
    "    1114: 114,\n",
    "    1116: 116,\n",
    "    1118: 118,\n",
    "    1121: 121,\n",
    "    1131: 131,\n",
    "    1149: 149,\n",
    "    1153: 153,\n",
    "    1164: 164,\n",
    "    1169: 169,\n",
    "    1175: 175,\n",
    "    1190: 190,\n",
    "    1194: 194,\n",
    "}\n",
    "\n",
    "\n",
    "# Some pages have not been properly OCR'ed (yet) so cannot be parsed.\n",
    "# Identify which charter numbers cannot be found because of these missing pages.\n",
    "missing_numbers = [\n",
    "    1,    # OHZ part 1, missing page 1\n",
    "    1258, # OHZ part 3, missing page 265\n",
    "    1270, # OHZ part 3, missing page 277\n",
    "    1471, # OHZ part 3, missing page 514\n",
    "    2052, # OHZ part 4, missing page 264\n",
    "    2581, # OHZ part 5, missing page 8\n",
    "    2584, # OHZ part 5, missing page 18\n",
    "    2598, # OHZ part 5, missing page 49\n",
    "    2599, # OHZ part 5, missing page 50\n",
    "    2600, # OHZ part 5, missing page 50\n",
    "    2601, # OHZ part 5, missing page 50\n",
    "    2602, # OHZ part 5, missing page 50\n",
    "    2603, # OHZ part 5, missing page 50\n",
    "    2604, # OHZ part 5, missing page 50\n",
    "    2605, # OHZ part 5, missing page 50\n",
    "    2606, # OHZ part 5, missing page 50\n",
    "    2607, # OHZ part 5, missing page 50\n",
    "    2608, # OHZ part 5, missing page 50\n",
    "    2629, # OHZ part 5, missing page 57\n",
    "    2630, # OHZ part 5, missing page 57\n",
    "    2639, # OHZ part 5, missing page 65\n",
    "    2640, # OHZ part 5, missing page 65\n",
    "    2644, # OHZ part 5, missing page 75\n",
    "    2654, # OHZ part 5, missing page 84\n",
    "    2655, # OHZ part 5, missing page 84\n",
    "    2660, # OHZ part 5, missing page 89\n",
    "    2670, # OHZ part 5, missing page 101\n",
    "    2676, # OHZ part 5, missing page 108\n",
    "    2694, # OHZ part 5, missing page 147\n",
    "    2741, # OHZ part 5, missing page 222\n",
    "    2773, # OHZ part 5, missing page 265\n",
    "    2776, # OHZ part 5, missing page 267\n",
    "    2805, # OHZ part 5, missing page 302\n",
    "    2874, # OHZ part 5, missing page 377\n",
    "    2889, # OHZ part 5, missing page 393\n",
    "    2898, # OHZ part 5, missing page 400\n",
    "    2929, # OHZ part 5, missing page 438\n",
    "    2953, # OHZ part 5, missing page 463\n",
    "    2981, # OHZ part 5, missing page 494\n",
    "    2983, # OHZ part 5, missing page 496\n",
    "    2984, # OHZ part 5, missing page 496\n",
    "    3001, # OHZ part 5, missing page 514\n",
    "    3004, # OHZ part 5, missing page 517\n",
    "    3008, # OHZ part 5, missing page 521\n",
    "    3157, # OHZ part 5, missing page 674\n",
    "    3170, # OHZ part 5, missing page 688\n",
    "    3172, # OHZ part 5, missing page 690\n",
    "    3237, # OHZ part 5, missing page 748\n",
    "    3293, # OHZ part 5, missing page 808\n",
    "    3365, # OHZ part 5, missing page 888\n",
    "    3366, # OHZ part 5, missing page 888\n",
    "    3378, # OHZ part 5, missing page 898\n",
    "    3436, # OHZ part 5, missing page 979\n",
    "    3437, # OHZ part 5, missing page 979\n",
    "    3443, # OHZ part 5, missing page 988\n",
    "    3444, # OHZ part 5, missing page 990\n",
    "    3453, # OHZ part 5, missing page 1012\n",
    "    3460, # OHZ part 5, missing page 1023\n",
    "    3484, # OHZ part 5, missing page 1050\n",
    "    3521, # OHZ part 5, missing page 1098\n",
    "    3529, # OHZ part 5, missing page 1113\n",
    "    3536, # OHZ part 5, missing page 1137\n",
    "]\n",
    "\n",
    "missing_data = [\n",
    "    {\"charter_num\": 1, \"book_num\": 1, \"page_num\": 1},\n",
    "    {\"charter_num\": 1258, \"book_num\": 3, \"page_num\": 265},\n",
    "    {\"charter_num\": 1270, \"book_num\": 3, \"page_num\": 277},\n",
    "    {\"charter_num\": 1471, \"book_num\": 3, \"page_num\": 514},\n",
    "    {\"charter_num\": 2052, \"book_num\": 4, \"page_num\": 264},\n",
    "    {\"charter_num\": 2581, \"book_num\": 5, \"page_num\": 8},\n",
    "    {\"charter_num\": 2584, \"book_num\": 5, \"page_num\": 18},\n",
    "    {\"charter_num\": 2598, \"book_num\": 5, \"page_num\": 49},\n",
    "    {\"charter_num\": 2599, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2600, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2601, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2602, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2603, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2604, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2605, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2606, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2607, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2608, \"book_num\": 5, \"page_num\": 50},\n",
    "    {\"charter_num\": 2629, \"book_num\": 5, \"page_num\": 57},\n",
    "    {\"charter_num\": 2630, \"book_num\": 5, \"page_num\": 57},\n",
    "    {\"charter_num\": 2639, \"book_num\": 5, \"page_num\": 65},\n",
    "    {\"charter_num\": 2640, \"book_num\": 5, \"page_num\": 65},\n",
    "    {\"charter_num\": 2644, \"book_num\": 5, \"page_num\": 75},\n",
    "    {\"charter_num\": 2654, \"book_num\": 5, \"page_num\": 84},\n",
    "    {\"charter_num\": 2655, \"book_num\": 5, \"page_num\": 84},\n",
    "    {\"charter_num\": 2660, \"book_num\": 5, \"page_num\": 89},\n",
    "    {\"charter_num\": 2670, \"book_num\": 5, \"page_num\": 101},\n",
    "    {\"charter_num\": 2676, \"book_num\": 5, \"page_num\": 108},\n",
    "    {\"charter_num\": 2694, \"book_num\": 5, \"page_num\": 147},\n",
    "    {\"charter_num\": 2741, \"book_num\": 5, \"page_num\": 222},\n",
    "    {\"charter_num\": 2773, \"book_num\": 5, \"page_num\": 265},\n",
    "    {\"charter_num\": 2776, \"book_num\": 5, \"page_num\": 267},\n",
    "    {\"charter_num\": 2805, \"book_num\": 5, \"page_num\": 302},\n",
    "    {\"charter_num\": 2874, \"book_num\": 5, \"page_num\": 377},\n",
    "    {\"charter_num\": 2889, \"book_num\": 5, \"page_num\": 393},\n",
    "    {\"charter_num\": 2898, \"book_num\": 5, \"page_num\": 400},\n",
    "    {\"charter_num\": 2929, \"book_num\": 5, \"page_num\": 438},\n",
    "    {\"charter_num\": 2953, \"book_num\": 5, \"page_num\": 463},\n",
    "    {\"charter_num\": 2981, \"book_num\": 5, \"page_num\": 494},\n",
    "    {\"charter_num\": 2983, \"book_num\": 5, \"page_num\": 496},\n",
    "    {\"charter_num\": 2984, \"book_num\": 5, \"page_num\": 496},\n",
    "    {\"charter_num\": 3001, \"book_num\": 5, \"page_num\": 514},\n",
    "    {\"charter_num\": 3004, \"book_num\": 5, \"page_num\": 517},\n",
    "    {\"charter_num\": 3008, \"book_num\": 5, \"page_num\": 521},\n",
    "    {\"charter_num\": 3157, \"book_num\": 5, \"page_num\": 674},\n",
    "    {\"charter_num\": 3170, \"book_num\": 5, \"page_num\": 688},\n",
    "    {\"charter_num\": 3172, \"book_num\": 5, \"page_num\": 690},\n",
    "    {\"charter_num\": 3237, \"book_num\": 5, \"page_num\": 748},\n",
    "    {\"charter_num\": 3293, \"book_num\": 5, \"page_num\": 808},\n",
    "    {\"charter_num\": 3365, \"book_num\": 5, \"page_num\": 888},\n",
    "    {\"charter_num\": 3366, \"book_num\": 5, \"page_num\": 888},\n",
    "    {\"charter_num\": 3378, \"book_num\": 5, \"page_num\": 898},\n",
    "    {\"charter_num\": 3436, \"book_num\": 5, \"page_num\": 979},\n",
    "    {\"charter_num\": 3437, \"book_num\": 5, \"page_num\": 979},\n",
    "    {\"charter_num\": 3443, \"book_num\": 5, \"page_num\": 988},\n",
    "    {\"charter_num\": 3444, \"book_num\": 5, \"page_num\": 990},\n",
    "    {\"charter_num\": 3453, \"book_num\": 5, \"page_num\": 1012},\n",
    "    {\"charter_num\": 3460, \"book_num\": 5, \"page_num\": 1023},\n",
    "    {\"charter_num\": 3484, \"book_num\": 5, \"page_num\": 1050},\n",
    "    {\"charter_num\": 3521, \"book_num\": 5, \"page_num\": 1098},\n",
    "    {\"charter_num\": 3529, \"book_num\": 5, \"page_num\": 1113},\n",
    "    {\"charter_num\": 3536, \"book_num\": 5, \"page_num\": 1137},\n",
    "]\n",
    "\n",
    "# keep track of pages missing in the hOCR output so that OHZ index reference to missing pages can be identified\n",
    "missing_pages = defaultdict(list)\n",
    "\n",
    "for book_num in range(1,6):\n",
    "    prev_page_num = 0\n",
    "    book_name = \"OHZ\" + str(book_num)\n",
    "    hocr_dir = \"hOCR/\" + book_name\n",
    "    for page_path, page_num in parse_hocr_files.get_hocr_files(hocr_dir):\n",
    "        while prev_page_num != page_num - 1:\n",
    "            missing_pages[book_num] += [prev_page_num+1]\n",
    "            prev_page_num += 1\n",
    "        #print(prev_page_num, page_num)\n",
    "        prev_page_num = page_num\n",
    "\n",
    "print(missing_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read place name list\n",
    "\n",
    "The list of place names is based on the OHZ Index that has place names as well as person names. The excel sheet contains a column with sets of place names separated by an empty cell. \n",
    "\n",
    "- The first name in the set tends to be the modern name/spelling (although for some places there is no modern name/spelling) \n",
    "- The other names in the set are variant names\n",
    "\n",
    "There are additional columns:\n",
    "\n",
    "- one column with country code labels indicating in which modern day country the place is geographically located.\n",
    "- one column with comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "\n",
    "class PlaceIndex(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.has_variants = defaultdict(dict)\n",
    "        self.is_variant_of = defaultdict(dict)\n",
    "        self.placename_ngram_index = defaultdict(dict)\n",
    "        self.placename_info = {}\n",
    "        self.non_placename = {}\n",
    "        \n",
    "    def add_placename(self, placename, placename_id, country, note):\n",
    "        self.placename_info[placename] = {\n",
    "            \"placename\": placename,\n",
    "            \"placename_id\": placename_id,\n",
    "            \"country\": country,\n",
    "            \"note\": note\n",
    "        }\n",
    "        \n",
    "    def add_variant(self, pref_name, variant_name, variant_id):\n",
    "        self.has_variants[pref_name][variant_name] = variant_id\n",
    "        self.is_variant_of[variant_name][pref_name] = variant_id\n",
    "\n",
    "    def index_placename_ngrams(self, place_name, ngram_size=3):\n",
    "        for ngram in get_ngrams(place_name, ngram_size):\n",
    "            self.placename_ngram_index[ngram][place_name] = True\n",
    "\n",
    "    def is_ambiguous(self, placename_string, is_variant_of):\n",
    "        if isinstance(get_preferred_placename(self, variant_placename), str):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def get_preferred_placename(self, variant_placename):\n",
    "        if variant_placename not in self.is_variant_of:\n",
    "            return None\n",
    "        preferred_placenames = list(self.is_variant_of[variant_placename].keys())\n",
    "        if len(preferred_placenames) == 1:\n",
    "            return preferred_placenames[0]\n",
    "        else:\n",
    "            return preferred_placenames\n",
    "\n",
    "    def add_non_placename(self, non_placename):\n",
    "        self.non_placename[non_placename] = 1\n",
    "        \n",
    "    def remove_non_placename(self, non_placename):\n",
    "        del self.non_placename[non_placename]\n",
    "\n",
    "    def is_non_placename(self, non_placename):\n",
    "        return non_placename in self.non_placename\n",
    "        \n",
    "    def is_placename(self, non_placename):\n",
    "        return non_placename in self.is_variant_of\n",
    "        \n",
    "def get_ngrams(term, ngram_size=3):\n",
    "    for start_index in range(0, len(term) - (ngram_size-1)):\n",
    "        yield term.lower()[start_index:start_index+ngram_size]\n",
    "\n",
    "def index_place_ngrams(place_name, ngram_size=3):\n",
    "    #print(\"Place name:\", place_name)\n",
    "    for ngram in get_ngrams(place_name, ngram_size):\n",
    "        place_ngram_index[ngram][place_name] = True\n",
    "\n",
    "def index_row(row, placename_index, preferred_placename):\n",
    "    row = {cell.column: cell.value for cell in row}\n",
    "    if row['B'] == \"Plaatsnaam\" or not row['B']:\n",
    "        preferred_placename = None\n",
    "    else:\n",
    "        placename_id = row['A']\n",
    "        placename_string = str(row['B'].strip())\n",
    "        if not preferred_placename:\n",
    "            preferred_placename = placename_string\n",
    "            country = row['C']\n",
    "            note = row['D']\n",
    "            placename_index.add_placename(preferred_placename, placename_id, country, note)\n",
    "        placename_index.add_variant(preferred_placename, placename_string, placename_id)\n",
    "        if len(placename_string) >= 4:\n",
    "            placename_index.index_placename_ngrams(placename_string)\n",
    "    return preferred_placename\n",
    "        \n",
    "def create_placename_index(placename_excel_file):\n",
    "    wb2 = load_workbook(placename_excel_file)\n",
    "    ws = wb2.active\n",
    "    placename_index = PlaceIndex()\n",
    "    preferred_placename = None\n",
    "    for row in ws:\n",
    "        preferred_placename = index_row(row, placename_index, preferred_placename)\n",
    "    return placename_index\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [2]\n",
    "intersect = set(a) & set(b)\n",
    "len(intersect) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching of Place Names\n",
    "\n",
    "Fuzzy matching on multiple criteria is an effective way to deal with OCR mistakes. Depending on the quality of the OCR, the thresholds for the various criteria can be high or low. With lower thresholds, more candidates are found, with more uncertainty. The advantage of using multiple criteria is that the uncertainty introduced by individual criteria is alleviated by the complementary signal of the other criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_ranked_candidates(candidate_name, ranked_candidates):\n",
    "    # Multiple variants of the same place may be fuzzy matches.\n",
    "    # In that case, select only the best matching one. \n",
    "    ranked_preferred_placenames = []\n",
    "    selected_candidates = []\n",
    "    for ranked_candidate in ranked_candidates:\n",
    "        if ranked_candidate[\"total\"] < 2: #len(candidate_name):\n",
    "            continue\n",
    "        preferred_placenames = placename_index.is_variant_of[ranked_candidate[\"candidate\"]].keys()\n",
    "        new_placenames = [placename for placename in preferred_placenames if placename not in ranked_preferred_placenames]\n",
    "        if len(new_placenames) == 0:\n",
    "            continue\n",
    "        selected_candidates += [ranked_candidate[\"candidate\"]] \n",
    "        ranked_preferred_placenames += new_placenames\n",
    "    return selected_candidates\n",
    "\n",
    "def is_candidate_ngram_placename_match(candidate_name, placename):\n",
    "    # Pruning step:\n",
    "    # - discard candidates that are much longer or shorter strings\n",
    "    if abs(len(placename) - len(candidate_name)) >= 3:\n",
    "        return False\n",
    "    # - discard candidates that start with a different initial (crude but effective)\n",
    "    if placename[0].lower() != candidate_name[0].lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_ngram_placenames(candidate_name, placename_index, ngram):\n",
    "    placenames = placename_index.placename_ngram_index[ngram].keys()\n",
    "    return [placename for placename in placenames if is_candidate_ngram_placename_match(candidate_name, placename)]\n",
    "\n",
    "def select_ngram_candidate_placenames(candidate_name, place_index, fuzzy_matcher):\n",
    "    ngrams = [ngram for ngram in get_ngrams(candidate_name) if ngram in placename_index.placename_ngram_index]\n",
    "    placenames = [placename for ngram in ngrams for placename in get_ngram_placenames(candidate_name, placename_index, ngram)]\n",
    "    placenames = fuzzy_matcher.filter_candidates(list(set(placenames)), candidate_name, 2)\n",
    "    #print(\"Ngram candidates:\", placenames)\n",
    "    return [placename for placename in placenames if is_candidate_ngram_placename_match(candidate_name, placename)]\n",
    "\n",
    "def find_fuzzy_placename_candidates(candidate_name, placename_index, fuzzy_matcher):\n",
    "    candidate_places = Counter()\n",
    "    ngram_candidate_placenames = select_ngram_candidate_placenames(candidate_name, place_index, fuzzy_matcher)\n",
    "    ranked_candidates = fuzzy_matcher.rank_candidates(ngram_candidate_placenames, candidate_name, ngram_size=2)\n",
    "    #print(\"candidate_name\", candidate_name)\n",
    "    #print(\"ranked_candidates:\", ranked_candidates)\n",
    "    #print()\n",
    "    selected_candidates = select_best_ranked_candidates(candidate_name, ranked_candidates)\n",
    "    return selected_candidates\n",
    "\n",
    "def is_fuzzy_candidate(candidate, placename_index):\n",
    "    if placename_index.is_non_placename(candidate):\n",
    "        # If candidate was established earlier to be not a placename, skip it.\n",
    "        # This is an efficiency/pruning step to avoid checking the same \n",
    "        # non-matches over and over.\n",
    "        return False\n",
    "    if placename_index.is_placename(candidate):\n",
    "        # Exact matches are dealt with elsewhere, skip here.\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_fuzzy_candidates(candidates, placename_index):\n",
    "    return [candidate for candidate in set(candidates) if is_fuzzy_candidate(candidate, placename_index)]\n",
    "    \n",
    "def find_fuzzy_placename_matches(text, placename_index, fuzzy_matcher):\n",
    "    candidates = re.findall(r\"\\b[A-Z]\\w+\", text)\n",
    "    candidates += [match[0] for match in re.findall(r\"\\b([A-Z]\\w+([ -][A-Z]\\w+)*)\", text) if match[0] not in candidates]\n",
    "    fuzzy_candidates = filter_fuzzy_candidates(candidates, placename_index)\n",
    "    fuzzy_place_matches = []\n",
    "    for fuzzy_candidate in fuzzy_candidates:\n",
    "        if len(fuzzy_candidate) < 4:\n",
    "            continue\n",
    "        candidate_places = find_fuzzy_placename_candidates(fuzzy_candidate, placename_index, fuzzy_matcher)\n",
    "        if len(candidate_places) == 0:\n",
    "            # candidate doesn't match with any known placename, so register as non placename\n",
    "            # for later pruning.\n",
    "            placename_index.add_non_placename(fuzzy_candidate)\n",
    "        else:\n",
    "            candidate_places = [{\"placename_variant\": placename, \"preferred_placename\": placename_index.get_preferred_placename(placename)} for placename in candidate_places]\n",
    "            fuzzy_place_matches += [{\"fuzzy_place_string\": fuzzy_candidate, \"candidate_places\": candidate_places}]\n",
    "    return fuzzy_place_matches\n",
    "\n",
    "char_match_threshold=0.7\n",
    "ngram_threshold=0.5\n",
    "levenshtein_threshold=0.8\n",
    "fuzzy_matcher = FuzzyMatcher(char_match_threshold=char_match_threshold, ngram_threshold=ngram_threshold, levenshtein_threshold=levenshtein_threshold)\n",
    "placename_index.non_placename = {}\n",
    "test_text = \"Origineel niet voorhanden. Afschriften: B (begin 14e e.) Staatsarchief Hannover, Kopiar II-4r van het domkapittel Hanburg (later Bremen), fol. 62 1, nr. 84, ad 1706, verbrand in het bombardement op Hannover van 1943 okt. 8/9. ? C (eind 15e e.) Ibidem, Kopiar 1-43 van het zelfde kapittel, fol. 84 v, nr. 92, op dezelfde wijze verbrand. ? D (eind 16e e.) Erpold Lindenbrog, Privilegia archiecclesie Hammaburgensis, hs. verbrand in de stadsbrand van Hamburg van 1842. \"\n",
    "\n",
    "fuzzy_matches = find_fuzzy_placename_matches(test_text, placename_index, fuzzy_matcher)\n",
    "print(fuzzy_matches)\n",
    "\n",
    "test_text = \"Drukken van DE: a. Schannat, Corpus Fuld., p. 316, nr. 70, naar E. ? b. Miraeus-Foppens, Opera dipl., HI, p. 8, nr. 5, $ 70, v??r c. 800, naar a. ? c. Dronke, Traditiones Fuldenses, D. 44, cap. 7, nr. 23, naar D; p. 51, nr. 124, naar E. ? d. Van den Bergh, Handboek, p. 266, nr. 70, ad [2e helft 8e e.], naar ab. ? e. Van den Bergh, OHZ, L, nr. 9, p. Gen 10, $ 23 en 124, c. eind 8e e., maar c. ? JS. Friedl?nder, Ostfriesisches UB,    p. 787, in nr. 3, naar D; b. 792, in nr. 9, naar E. ? g. Bunte, Untersuchungen, p. 31 en 40, nrs. 23 en 124, naar c. \" \n",
    "\n",
    "fuzzy_matches = find_fuzzy_placename_matches(test_text, placename_index, fuzzy_matcher)\n",
    "print(fuzzy_matches)\n",
    "\n",
    "test_text = \"Waterlandie Prenominati Aribone Traiectensem Westfrisie\"\n",
    "\n",
    "fuzzy_matches = find_fuzzy_placename_matches(test_text, placename_index, fuzzy_matcher)\n",
    "print(fuzzy_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index charters\n",
    "\n",
    "Gather paragraphs by charter, identify place name mentions and index at charter level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_paragraphs_by_charter(charter_num):\n",
    "    query = {\"query\": {\"match\": {\"charter_number\": charter_num}}, \"size\": 10000}\n",
    "    response = es.search(index=\"retroboeken\", doc_type=\"ohz-paragraph\", body=query)\n",
    "    return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "def parse_paragraphs(paragraphs):\n",
    "    charter = {\n",
    "        \"paragraphs\": [],\n",
    "        \"exact_place_matches\": [],\n",
    "        \"fuzzy_place_matches\": [],\n",
    "        \"page_numbers\": [],\n",
    "    }\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph[\"page_num\"] not in charter[\"page_numbers\"]:\n",
    "            charter[\"page_numbers\"] += [paragraph[\"page_num\"]]\n",
    "        charter[\"charter_number\"] = paragraph[\"charter_number\"]\n",
    "        charter[\"book_number\"] = paragraph[\"book_num\"]\n",
    "        charter[\"paragraphs\"] += [\n",
    "            {\n",
    "                \"lines\": paragraph[\"line_texts\"],\n",
    "                \"line_numbers\": paragraph[\"line_numbers\"],\n",
    "                \"book_number\": paragraph[\"book_num\"],\n",
    "                \"page_number\": paragraph[\"page_num\"],\n",
    "                \"paragraph_number\": paragraph[\"paragraph_num\"],\n",
    "                \"merged_text\": paragraph[\"merged_text\"],\n",
    "                \"type\": paragraph[\"type\"],\n",
    "            }\n",
    "        ]\n",
    "    charter[\"page_numbers\"].sort()\n",
    "    return charter\n",
    "\n",
    "def is_ambiguous(placename_string, is_variant_of):\n",
    "    if len(list(is_variant_of[placename_string].keys())) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_placename_match(placename_string, charter_num, paragraph, match):\n",
    "    placename_pref = list(is_variant_of[placename_string].keys())\n",
    "    placename_ambiguity = \"ambiguous\"\n",
    "    if len(placename_pref) == 1:\n",
    "        placename_pref = placename_pref[0]\n",
    "        placename_ambiguity = \"unambiguous\"\n",
    "    placename_match = {\n",
    "        \"placename_string\": placename_string,\n",
    "        \"placename_pref\": placename_pref,\n",
    "        \"charter\": charter_num,\n",
    "        \"book_number\": paragraph[\"book_number\"],\n",
    "        \"page_number\": paragraph[\"page_number\"], \n",
    "        \"paragraph_number\": paragraph[\"paragraph_number\"], \n",
    "        \"line_number\": match[\"line_number\"],\n",
    "        \"match_type\": match[\"match_type\"],\n",
    "        \"placename_ambiguity\": is_ambiguous(placename_string, is_variant_of),\n",
    "    }\n",
    "    if match[\"match_type\"] == \"cross_line\":\n",
    "        placename_match[\"match_parts\"] = match[\"match_parts\"]\n",
    "    return placename_match\n",
    "\n",
    "def lookup_placename_in_paragraph(charter_num, paragraph, placename_string):\n",
    "    place_matches = []\n",
    "    for line_index, line in enumerate(paragraph[\"lines\"]):\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        match_info = lookup_placename_in_single_line(placename_string, paragraph, line_index)\n",
    "        if not match_info and line[-1] == \"-\" and len(paragraph[\"lines\"]) > line_index+1:\n",
    "            match_info = lookup_placename_in_merged_line(placename_string, paragraph, line_index)\n",
    "        if match_info:\n",
    "            placename_match = create_placename_match(placename_string, charter_num, paragraph, match_info)\n",
    "            placename_matches += [placename_match]\n",
    "    return placename_matches\n",
    "\n",
    "def fuzzy_lookup_place_in_paragraph(charter_num, paragraph, place_string):\n",
    "    fuzzy_matches = []\n",
    "    for line_index, line in enumerate(paragraph[\"lines\"]):\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        for fuzzy_match in find_fuzzy_placename_matches(line, placename_index, fuzzy_matcher):\n",
    "            fuzzy_match[\"charter\"] = charter_num\n",
    "            fuzzy_match[\"book_number\"] = paragraph[\"book_number\"]\n",
    "            fuzzy_match[\"page_number\"] = paragraph[\"page_number\"]\n",
    "            fuzzy_match[\"paragraph_number\"] = paragraph[\"paragraph_number\"]\n",
    "            fuzzy_match[\"match_type\"] = \"single_line\"\n",
    "            fuzzy_match[\"line_number\"] = paragraph[\"line_numbers\"][line_index]\n",
    "            fuzzy_matches += [fuzzy_match]\n",
    "    return fuzzy_matches\n",
    "\n",
    "def term_in_text(term, text):\n",
    "    if term not in text:\n",
    "        return False\n",
    "    if re.search(r\"\\W\" + to_regex_string(term) + r\"\\W\", text):\n",
    "        return True\n",
    "    elif re.search(r\"\\W\" + to_regex_string(term) + r\"$\", text):\n",
    "        return True\n",
    "    elif re.search(r\"^\" + to_regex_string(term) + r\"\\W\", text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def to_regex_string(string):\n",
    "    return string.replace(\"[\", r\"\\[\").replace(\"]\", r\"\\]\").replace(\"(\", r\"\\(\").replace(\")\", r\"\\)\").replace(\"*\", r\"\\*\")\n",
    "\n",
    "def lookup_placename_in_single_line(place_string, paragraph, line_index):\n",
    "    if term_in_text(place_string, paragraph[\"lines\"][line_index]):\n",
    "        return {\n",
    "            \"match_type\": \"single_line\", \n",
    "            \"line_number\": paragraph[\"line_numbers\"][line_index], \n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lookup_placename_in_merged_line(place_string, paragraph, line_index):\n",
    "    if term_in_text(place_string, paragraph[\"lines\"][line_index+1]):\n",
    "        return None\n",
    "    merged_line = paragraph[\"lines\"][line_index][:-1] + paragraph[\"lines\"][line_index+1].strip()\n",
    "    if term_in_text(place_string, merged_line):\n",
    "        offset = merged_line.index(place_string)\n",
    "        curr_line_part = paragraph[\"lines\"][line_index][offset:]\n",
    "        next_line_part = place_string[(len(curr_line_part)-1):]\n",
    "        return {\n",
    "            \"match_type\": \"cross_line\", \n",
    "            \"line_number\": [paragraph[\"line_numbers\"][line_index], paragraph[\"line_numbers\"][line_index+1]], \n",
    "            \"match_parts\": [curr_line_part, next_line_part]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_placename_matches(charter, charter_text, is_variant_of):\n",
    "    # Iterate over all placenames in the list and scan the charter for each of these.\n",
    "    for place_string in is_variant_of:\n",
    "        # Skip the small number of very short placenames, like A and Le as they\n",
    "        # results in enormous amounts of mostly incorrect matches. \n",
    "        if len(place_string) < 3:\n",
    "            continue\n",
    "        # Check if the placename occurs in the overall charter text as string.\n",
    "        # If not, skip further analysis. If so, check that it occurs as single\n",
    "        # or multiterm mention and not as part of a larger term.\n",
    "        if place_string in charter_text:\n",
    "            for paragraph in charter[\"paragraphs\"]:\n",
    "                exact_matches = lookup_place_in_paragraph(charter[\"charter_number\"], paragraph, place_string)\n",
    "                charter[\"exact_place_matches\"] += exact_matches\n",
    "    for paragraph in charter[\"paragraphs\"]:\n",
    "        fuzzy_matches = fuzzy_lookup_place_in_paragraph(charter[\"charter_number\"], paragraph, place_string)\n",
    "        charter[\"fuzzy_place_matches\"] += fuzzy_matches\n",
    "\n",
    "# Some placenames mentions are incomplete, where bits of the text is missing.\n",
    "# In these cases, the missing part is replace by [...] with the number of \n",
    "# dots indicating the estimates number of missing characters.\n",
    "is_incomplete_variant_of = {place: is_variant_of[place] for place in is_variant_of if re.search(r\"\\[.*\\]\",place)}\n",
    "# Other placenames have no square brackets, suggesting they are complete.\n",
    "is_complete_variant_of = {place: is_variant_of[place] for place in is_variant_of if not re.search(r\"\\[.*\\]\",place)}\n",
    "\n",
    "\n",
    "# remove index to get rid of incorrect charters from previous iterations\n",
    "if es.indices.exists(index='ohz-test'):\n",
    "    es.indices.delete(index='ohz-test')\n",
    "\n",
    "for charter_num in range(1,3538):\n",
    "    paragraphs = get_paragraphs_by_charter(charter_num)\n",
    "    if len(paragraphs) == 0:\n",
    "        print(\"No paragraphs for charter\", charter_num)\n",
    "        continue\n",
    "    charter = parse_paragraphs(paragraphs)\n",
    "    charter_text = \" \".join([paragraph[\"merged_text\"] for paragraph in charter[\"paragraphs\"]])\n",
    "    # First, look for incomplete placenames in original text\n",
    "    add_placename_matches(charter, charter_text, is_incomplete_variant_of)\n",
    "    # Remove all square brackets from the charter text, which incidate uncertainty or missing text. \n",
    "    charter_text = charter_text.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    # Second, look for complete placenames in modified charter text.\n",
    "    add_placename_matches(charter, charter_text, is_complete_variant_of)\n",
    "    for charter_number in charter[\"charter_number\"]:\n",
    "        es.index(index=\"ohz-test\", doc_type=\"ohz-charter\", id=charter_number, body=charter)\n",
    "    if charter_num % 50 == 0:\n",
    "        print(charter_num)\n",
    "        #print(json.dumps(charter, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicating Charter Dates\n",
    "\n",
    "Charter titles contain a date, a charter number and optionally a location. Charter numbers have already been identified. the next step is to interpret the date strings so that they can be queried numerically.\n",
    "\n",
    "Issues to deal with:\n",
    "\n",
    "- Certain dates are ranges, e.g. \"726 okt. 21 - 727 mei 13\" (charter 2 OHZ 1 p. 2), so have a start and end date \n",
    "- Certain dates are non-numerically expressed, e.g. \"Eind 8e eeuw? - 817\" (charter 7, OHZ 1 p. 12)\n",
    "- Certain dates are estimates, e.g. \"c. 2de helft 9e eeuw\" (charter 23, OHZ 1 p. ) or \"Waarschijnlijk 941 nov. 28-dec.24\"\n",
    "- Certain dates are conjectures (\"Tussen rechte haken staan de dateringselementen die conjecturaal zijn.\" OHZ 1, p. XV), indicated by square brackets.\n",
    "\n",
    "Choice to make:\n",
    "\n",
    "- Select a specific century for century-based estimates?\n",
    "- Select a sort date (e.g. a single date on which all charters can be sorted)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check assumptions:\n",
    "# - charter date and charter number are always separated by at least 4 whitespaces\n",
    "# - exceptions:\n",
    "#   - charters 82 and 83\n",
    "#   - charters 139 and 140\n",
    "#   - charters 849 and 850\n",
    "#   - charters 2535 and 2536\n",
    "#   - charter 1906 (printed together with 1903 even though 1903 is separately listed earlier, see OHZ 4 p. 84 and p. 89)\n",
    "\n",
    "# Check data:\n",
    "# - charter date string is never more than 36 characters per line\n",
    "# - year constraints: between 719 and 1299\n",
    "#   - part 1: between 719 and 1222\n",
    "#   - part 2: between 1222 and 1256\n",
    "#   - part 3: between 1256 and 1278\n",
    "#   - part 4: between 1278 and 1291\n",
    "#   - part 5: between 1291 and 1299\n",
    "\n",
    "from datetime import date as to_date\n",
    "\n",
    "year_constraints = {\n",
    "    1: {\"min\": 719, \"max\": 1222},\n",
    "    2: {\"min\": 1222, \"max\": 1256},\n",
    "    3: {\"min\": 1256, \"max\": 1278},\n",
    "    4: {\"min\": 1278, \"max\": 1291},\n",
    "    5: {\"min\": 1291, \"max\": 1299},\n",
    "}\n",
    "\n",
    "# There are a few charters that have complex titles that deviate \n",
    "# strongly from the typical charter title format.\n",
    "# Instead of writing a large chunk of code to capture these exceptions, \n",
    "# I've chosen to write out the proper date information for them.\n",
    "exceptions = {\n",
    "    82: {\n",
    "        \"sort_year\": 1057, \"start_year\": 1057, \"end_year\": 1057, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1057-10-30\", \"start_date\": \"1057-10-30\", \"end_date\": \"1057-10-30\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    83: {\n",
    "        \"sort_year\": 1058, \"start_year\": 1058, \"end_year\": 1058, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1058-06-25\", \"start_date\": \"1058-06-25\", \"end_date\": \"1058-06-25\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    139: {\n",
    "        \"sort_year\": 1156, \"start_year\": 1156, \"end_year\": 1156, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1156-01-01\", \"start_date\": \"1156-01-01\", \"end_date\": \"1156-01-01\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_year\",\n",
    "    },\n",
    "    140: {\n",
    "        \"sort_year\": 1156, \"start_year\": 1156, \"end_year\": 1156, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1156-01-01\", \"start_date\": \"1156-01-01\", \"end_date\": \"1156-01-01\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_year_month\",\n",
    "    },\n",
    "    473: {\n",
    "        \"sort_year\": 1226, \"start_year\": 1226, \"end_year\": 1227, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1226-01-01\", \"start_date\": \"1226-06-19\", \"end_date\": \"1226-01-01\",\n",
    "        \"date_certainty\": \"uncertain\", \"date_specifity\": \"circa_date\",\n",
    "    },\n",
    "    849: {\n",
    "        \"sort_year\": 1250, \"start_year\": 1250, \"end_year\": 1250, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1250-05-19\", \"start_date\": \"1250-05-19\", \"end_date\": \"1250-05-19\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    850: {\n",
    "        \"sort_year\": 1250, \"start_year\": 1250, \"end_year\": 1250, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1250-05-19\", \"start_date\": \"1250-05-19\", \"end_date\": \"1250-05-19\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    1906: {\n",
    "        \"sort_year\": 1280, \"start_year\": 1280, \"end_year\": 1280, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1280-05-13\", \"start_date\": \"1280-05-13\", \"end_date\": \"1280-05-13\",\n",
    "        \"date_certainty\": \"certain\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    2535: {\n",
    "        \"sort_year\": 1290, \"start_year\": 1290, \"end_year\": 1290, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1290-11-13\", \"start_date\": \"1280-11-13\", \"end_date\": \"1280-11-13\",\n",
    "        \"date_certainty\": \"false\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "    2536: {\n",
    "        \"sort_year\": 1290, \"start_year\": 1290, \"end_year\": 1290, \"year_type\": \"exact\",\n",
    "        \"sort_date\": \"1290-11-13\", \"start_date\": \"1280-11-13\", \"end_date\": \"1280-11-13\",\n",
    "        \"date_certainty\": \"false\", \"date_specifity\": \"specific_date\",\n",
    "    },\n",
    "}\n",
    "\n",
    "full_date_pattern = r\"\\b(\\d{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) (\\d+)\\b\"\n",
    "\n",
    "def get_charter(charter_num):\n",
    "    if es.exists(index=\"ohz\", doc_type=\"ohz-charter\", id=charter_num):\n",
    "        response = es.get(index=\"ohz\", doc_type=\"ohz-charter\", id=charter_num)\n",
    "        return response[\"_source\"]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Make sure that month references always have the same format.\n",
    "def normalize_months(date_string):\n",
    "    date_string = date_string.replace(\"januari\", \"jan\")\n",
    "    date_string = date_string.replace(\"februari\", \"feb\")\n",
    "    date_string = date_string.replace(\"febr\", \"feb\")\n",
    "    date_string = date_string.replace(\"maart\", \"mrt\")\n",
    "    date_string = date_string.replace(\"april\", \"apr\")\n",
    "    date_string = date_string.replace(\"augustus\", \"aug\")\n",
    "    date_string = date_string.replace(\"september\", \"sept\")\n",
    "    date_string = date_string.replace(\"oktober\", \"okt\")\n",
    "    date_string = date_string.replace(\"november\", \"nov\")\n",
    "    date_string = date_string.replace(\"december\", \"dec\")\n",
    "    date_string = re.sub(r\"\\b(jan|feb|mrt|apr|aug|sept|okt|nov|dec):\", r\"\\1.\", date_string)\n",
    "    date_string = re.sub(r\"\\b(jan|feb|mrt|apr|aug|sept|okt|nov|dec) \", r\"\\1. \", date_string)\n",
    "    date_string = re.sub(r\"\\b(jan|feb|mrt|apr|aug|sept|okt|nov|dec)$\", r\"\\1.\", date_string)\n",
    "    return date_string\n",
    "\n",
    "# Map month name to month number\n",
    "def map_month(month_string):\n",
    "    months = [\"jan.\", \"feb.\", \"mrt.\", \"apr.\", \"mei\", \"juni\", \"juli\", \"aug.\", \"sept.\", \"okt.\", \"nov.\", \"dec.\"]\n",
    "    if month_string == \"febr.\":\n",
    "        month_string = \"feb.\"\n",
    "    return months.index(month_string) + 1\n",
    "\n",
    "# Dates examples:\n",
    "# - [Waarschijnlijk 983] jan. 9\n",
    "# - 98[5] juni 26\n",
    "# - 985 aug. 25\n",
    "# - [981-985] okt. 2 \n",
    "def is_specific_date(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    year = int(deconfuse(match.group(1)))\n",
    "    month = map_month(match.group(2))\n",
    "    day = int(deconfuse(match.group(3)))\n",
    "    if year < MINIMUM_YEAR or year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR year:\", [year, month, day], date_string)\n",
    "        return False\n",
    "    if day > 31:\n",
    "        print(\"DATE ERROR day:\", [year, month, day], date_string)\n",
    "        return False\n",
    "    return (year, month, day)\n",
    "    \n",
    "def has_specific_year_and_date_range(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})(-|\\?|-\\?| \\?|\\? | \\? )(jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    start_day = int(match.group(3))\n",
    "    end_year = int(match.group(1))\n",
    "    end_month = map_month(match.group(5))\n",
    "    end_day = int(match.group(6))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if start_day > 31:\n",
    "        print(\"DATE ERROR start_day:\", [start_year, start_month, start_day], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, start_day, end_year, end_month, end_day)\n",
    "    \n",
    "def has_specific_year_month_and_day_range(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})(-|\\?|-\\?| \\?|\\? | \\? )([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    start_day = int(match.group(3))\n",
    "    end_year = int(match.group(1))\n",
    "    end_month = map_month(match.group(2))\n",
    "    end_day = int(match.group(5))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if start_day > 31:\n",
    "        print(\"DATE ERROR start_day:\", [start_year, start_month, start_day], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, start_day, end_year, end_month, end_day)\n",
    "\n",
    "def has_year_range_and_specific_date(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4})(-|\\?|-\\?| \\?|\\? | \\? )([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(4))\n",
    "    start_day = int(match.group(5))\n",
    "    end_year = int(match.group(3))\n",
    "    end_month = map_month(match.group(4))\n",
    "    end_day = int(match.group(5))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if start_day > 31:\n",
    "        print(\"DATE ERROR start_day:\", [start_year, start_month, start_day], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, start_day, end_year, end_month, end_day)\n",
    "    \n",
    "def has_full_date_range(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})(-|\\?|-\\?| \\?|\\? | \\? )([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    start_day = int(match.group(3))\n",
    "    end_year = int(match.group(5))\n",
    "    end_month = map_month(match.group(6))\n",
    "    end_day = int(match.group(7))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if start_day > 31:\n",
    "        print(\"DATE ERROR start_day:\", [start_year, start_month, start_day], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, start_day, end_year, end_month, end_day)\n",
    "\n",
    "def has_year_month_and_full_date(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.)(-|\\?|-\\?| \\?|\\? | \\? )([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    end_year = int(match.group(4))\n",
    "    end_month = map_month(match.group(5))\n",
    "    end_day = int(match.group(6))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, end_year, end_month, end_day)\n",
    "\n",
    "def has_year_month_year_month(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.)(-|\\?|-\\?| \\?|\\? | \\? )([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.)$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    end_year = int(match.group(4))\n",
    "    end_month = map_month(match.group(5))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, end_year, end_month)\n",
    "\n",
    "def has_year_month_month_day(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.)(-|\\?|-\\?| \\?|\\? | \\? )(jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.) ([0-9gioz]{1,2})$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    start_month = map_month(match.group(2))\n",
    "    end_year = start_year\n",
    "    end_month = map_month(match.group(4))\n",
    "    end_day = int(match.group(5))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    if end_day > 31:\n",
    "        print(\"DATE ERROR end_day:\", [end_year, end_month, end_day], date_string)\n",
    "        return False\n",
    "    return (start_year, start_month, end_year, end_month, end_day)\n",
    "\n",
    "# year circa indictor month day\n",
    "\n",
    "def has_year_month(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4}) (jan\\.|feb\\.|febr\\.|mrt\\.|apr\\.|mei|juni|juli|aug\\.|sept\\.|okt\\.|nov\\.|dec\\.)$\", date_string)\n",
    "    if not match:\n",
    "        return False\n",
    "    year = int(match.group(1))\n",
    "    month = map_month(match.group(2))\n",
    "    if year < MINIMUM_YEAR or year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR year:\", [year, month], date_string)\n",
    "        return False\n",
    "    return (year, month)\n",
    "    \n",
    "def has_specific_year(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4})$\", deconfuse(date_string))\n",
    "    if not match:\n",
    "        return False\n",
    "    year = int(match.group(1))\n",
    "    if year < MINIMUM_YEAR or year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR year:\", [year], date_string)\n",
    "        return False\n",
    "    return year\n",
    "    \n",
    "def has_year_range(date_string):\n",
    "    match = re.match(r\"^([0-9gioz]{3,4})-([0-9gioz]{3,4})$\", deconfuse(date_string))\n",
    "    if not match:\n",
    "        return False\n",
    "    start_year = int(match.group(1))\n",
    "    end_year = int(match.group(2))\n",
    "    if start_year < MINIMUM_YEAR or start_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR start_year:\", [start_year], date_string)\n",
    "        return False\n",
    "    if end_year < MINIMUM_YEAR or end_year > MAXIMUM_YEAR:\n",
    "        print(\"DATE ERROR end_year:\", [end_year], date_string)\n",
    "        return False\n",
    "    return start_year, end_year\n",
    "    \n",
    "def make_proper_date(date_tuple):\n",
    "    \"\"\"\n",
    "    date_tuple should have three integers for year, month and date. \n",
    "    if there's only a year and month, assume first day of the month.\n",
    "    if there's only a year, assume first day of january.\n",
    "    \"\"\"\n",
    "    if len(date_tuple) == 1:\n",
    "        return to_date(date_tuple[0], 1, 1)\n",
    "    if len(date_tuple) == 2:\n",
    "        return to_date(date_tuple[0], date_tuple[1], 1)\n",
    "    else:\n",
    "        return to_date(date_tuple[0], date_tuple[1], date_tuple[2])\n",
    "\n",
    "def make_year_info(sort_year, end_year=None):\n",
    "    if not end_year:\n",
    "        end_year = sort_year\n",
    "    return {\"sort_year\": sort_year, \"start_year\": sort_year, \"end_year\": end_year}\n",
    "\n",
    "def parse_date_string(date_string, charter_num):\n",
    "    date_string = date_string.lower()\n",
    "    date_type = \"exact\"\n",
    "    date_certainty = \"certain\"\n",
    "    date_specificity = \"specific\"\n",
    "    date_modifier = \"\"\n",
    "    date_string = date_string.replace(\";\", \"]\")\n",
    "    date_string = date_string.replace(\"v??r\", \"voor\")\n",
    "    # remove initial single digit, e.g. \"1   1299 okt. 12\"\n",
    "    if re.match(r\"[0-9gioz] {2,}[0-9gioz]\", date_string):\n",
    "        date_string = re.sub(r\"^[0-9gioz] {2,}\", \"\", date_string)\n",
    "    # remove comma after year, e.g. \"1299, \"\n",
    "    if re.search(r\"[0-9gioz]{3,4},\", date_string):\n",
    "        date_string = re.sub(r\"([0-9gioz]{3,4}),\", r\"\\1\", date_string)\n",
    "    # remove falseness indicator\n",
    "    if \"<\" in date_string and \">\" in date_string:\n",
    "        date_string = date_string.replace(\"<\",\"\").replace(\">\",\"\")\n",
    "        date_certainty = \"unreliable\"\n",
    "    # remove uncertainty indicator\n",
    "    if \"[\" in date_string and \"]\" in date_string:\n",
    "        date_string = date_string.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        date_certainty = \"uncertain\"\n",
    "    # remove uncertainty indicator\n",
    "    if date_string[0] == \"[\" and date_string[-1] == \"]\":\n",
    "        date_certainty = \"uncertain\"\n",
    "        date_string = date_string[1:-1]\n",
    "    if date_string[0] == \"[\":\n",
    "        date_string = date_string[1:]\n",
    "        date_certainty = \"uncertain\"\n",
    "    # remove uncertainty terms, e.g. \"vermoedelijk|verm.|waarschijnlijk|ws.\"\n",
    "    if re.search(r\"\\b(vermoedelijk|verm.|waarschijnlijk|ws.) \", date_string):\n",
    "        date_string = re.sub(r\"\\b(vermoedelijk|verm.|waarschijnlijk|ws.) \", \"\", date_string)\n",
    "        date_certainty = \"uncertain\"\n",
    "    if re.search(r\" (vermoedelijk|verm.|waarschijnlijk|ws.)\\b\", date_string):\n",
    "        date_string = re.sub(r\" (vermoedelijk|verm.|waarschijnlijk|ws.)\\b\", \"\", date_string)\n",
    "        date_certainty = \"uncertain\"\n",
    "    # remove circa indicators, e.g. \"c.|ca.|circa\"\n",
    "    if re.search(r\"\\b(c.|ca.|circa) \", date_string):\n",
    "        date_string = re.sub(r\"\\b(c.|ca.|circa) \", \"\", date_string)\n",
    "        date_modifier = \"circa\"\n",
    "    # remove earlier/later indicators\n",
    "    if re.search(r\"\\b(enige tijd voor|kort voor|uiterlijk|voor) \", date_string):\n",
    "        date_string = re.sub(r\"\\b(enige tijd voor|kort voor|uiterlijk|voor) \", \"\", date_string)\n",
    "        date_modifier = \"earlier\"\n",
    "    if re.search(r\"\\b(kort na|na) \", date_string):\n",
    "        date_string = re.sub(r\"\\b(kort na|na) \", \"\", date_string)\n",
    "        date_modifier = \"later\"\n",
    "    if re.search(r\" of kort daarna\", date_string):\n",
    "        date_string = re.sub(r\" of kort daarna\", \"\", date_string)\n",
    "        date_modifier = \"or_later\"\n",
    "    if re.search(r\" of kort daarvoor\", date_string):\n",
    "        date_string = re.sub(r\" of kort daarvoor\", \"\", date_string)\n",
    "        date_modifier = \"or_earlier\"\n",
    "    # replace or indicator with start-end indicator\n",
    "    if re.search(r\" of \", date_string):\n",
    "        date_string = re.sub(r\" of \", \"-\", date_string)\n",
    "        if len(date_modifier) > 0:\n",
    "            date_modifier += \"_or_dates\"\n",
    "        else:\n",
    "            date_modifier = \"or_dates\"\n",
    "    date_string = normalize_months(date_string)\n",
    "    if is_specific_date(date_string):\n",
    "        date_tuple = is_specific_date(date_string)\n",
    "        year_info = make_year_info(date_tuple[0])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple)\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple)\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple)\n",
    "        year_info[\"date_specificity\"] = \"specific_date\"\n",
    "    elif has_full_date_range(date_string):\n",
    "        date_tuple = has_full_date_range(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[3])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[3:])\n",
    "        year_info[\"date_specificity\"] = \"range_date\"\n",
    "    elif has_specific_year_and_date_range(date_string):\n",
    "        date_tuple = has_specific_year_and_date_range(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[3])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[3:])\n",
    "        year_info[\"date_specificity\"] = \"specific_year_range_month_day\"\n",
    "    elif has_specific_year_month_and_day_range(date_string):\n",
    "        date_tuple = has_specific_year_month_and_day_range(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[3])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[3:])\n",
    "        year_info[\"date_specificity\"] = \"specifc_year_month_range_day\"\n",
    "    elif has_year_range_and_specific_date(date_string):\n",
    "        date_tuple = has_year_range_and_specific_date(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[3])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:3])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[3:])\n",
    "        year_info[\"date_specificity\"] = \"range_year_specific_month_day\"\n",
    "    # year month - year month day\n",
    "    elif has_year_month_and_full_date(date_string):\n",
    "        date_tuple = has_year_month_and_full_date(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[2])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[2:])\n",
    "        year_info[\"date_specificity\"] = \"specific_year_month_range_date\"\n",
    "    # year month - year month\n",
    "    elif has_year_month_year_month(date_string):\n",
    "        date_tuple = has_year_month_year_month(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[2])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[2:])\n",
    "        year_info[\"date_specificity\"] = \"range_year_month\"\n",
    "    # year month - month day\n",
    "    elif has_year_month_month_day(date_string):\n",
    "        date_tuple = has_year_month_month_day(date_string)\n",
    "        year_info = make_year_info(date_tuple[0], date_tuple[2])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[2:])\n",
    "        year_info[\"date_specificity\"] = \"specific_year_range_month_day\"\n",
    "    elif has_year_month(date_string):\n",
    "        date_tuple = has_year_month(date_string)\n",
    "        year_info = make_year_info(date_tuple[0])\n",
    "        year_info[\"sort_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"start_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"end_date\"] = make_proper_date(date_tuple[:2])\n",
    "        year_info[\"date_specificity\"] = \"specific_year_month\"\n",
    "    elif has_specific_year(date_string):\n",
    "        year = has_specific_year(date_string)\n",
    "        year_info = make_year_info(year)\n",
    "        year_info[\"sort_date\"] = make_proper_date([year])\n",
    "        year_info[\"start_date\"] = make_proper_date([year])\n",
    "        year_info[\"end_date\"] = make_proper_date([year])\n",
    "        year_info[\"date_specificity\"] = \"specific_year\"\n",
    "    elif has_year_range(date_string):\n",
    "        start_year, end_year = has_year_range(date_string)\n",
    "        year_info = make_year_info(start_year, end_year)\n",
    "        year_info[\"sort_date\"] = make_proper_date([start_year])\n",
    "        year_info[\"start_date\"] = make_proper_date([start_year])\n",
    "        year_info[\"end_date\"] = make_proper_date([end_year])\n",
    "        year_info[\"date_specificity\"] = \"range_year\"\n",
    "    else:\n",
    "        print(\"\\t\", charter_num, \"\\t\", date_string)\n",
    "        return None\n",
    "    year_info[\"date_certainty\"] = date_certainty\n",
    "    if \"date_specificity\" not in year_info:\n",
    "        year_info[\"date_specificity\"] = date_specificity\n",
    "        raise KeyError(\"Year info has no date_specificity:\", year_info, date_string)\n",
    "    year_info[\"date_string\"] = date_string\n",
    "    year_info[\"date_modifier\"] = date_modifier\n",
    "    return year_info\n",
    "\n",
    "def make_unknown_date_info(date_string):\n",
    "    return {\n",
    "        \"sort_year\": None, \"start_year\": None, \"end_year\": None,\n",
    "        \"sort_date\": None, \"start_date\": None, \"end_date\": None,\n",
    "        \"date_specificity\": \"unknown\", \"date_certainty\": \"unknown\", \n",
    "        \"date_modifier\": \"unknown\", \"date_string\": date_string,\n",
    "    }\n",
    "\n",
    "def get_title_paragraph(charter):\n",
    "    for paragraph in charter[\"paragraphs\"]:\n",
    "        if paragraph[\"type\"] == \"charter_title\":\n",
    "            return paragraph\n",
    "    return None\n",
    "\n",
    "def get_date_string(charter):\n",
    "    # the date of the charter is in the title paragraph\n",
    "    title_paragraph = get_title_paragraph(charter)\n",
    "    if not title_paragraph:\n",
    "        return \"\"\n",
    "    first_line = title_paragraph[\"lines\"][0]\n",
    "    parts = re.split(\" {4,}\", first_line.strip())\n",
    "    date_string = parts[0]\n",
    "    if len(title_paragraph[\"lines\"]) > 1:\n",
    "        #print(\"before: #{d}#\".format(d=date_string))\n",
    "        for line in title_paragraph[\"lines\"][1:]:\n",
    "            # remove anything that's not floating left\n",
    "            # e.g. \"                          van Saint-Aubert te Kamerijk\" in charter 3406\n",
    "            extra_date_string = re.sub(r\" {6,60}.*\", \"\", line)\n",
    "            if len(extra_date_string) > 0:\n",
    "                date_string += \" \" + extra_date_string[:40].strip()\n",
    "                date_string = date_string.strip()\n",
    "    return date_string\n",
    "\n",
    "MINIMUM_YEAR = 719\n",
    "MAXIMUM_YEAR = 1299\n",
    "\n",
    "date_found = 0\n",
    "date_not_found = 0\n",
    "date_skipped = 0\n",
    "charter_date = {}\n",
    "for charter_num in range(1,3537):\n",
    "    charter = get_charter(charter_num)\n",
    "    if not charter:\n",
    "        charter_date[charter_num] = make_unknown_date_info(\"\")\n",
    "        date_skipped += 1\n",
    "        continue\n",
    "    #print(\"parsing charter\", charter_num)\n",
    "    date_string = get_date_string(charter)\n",
    "    if charter_num in exceptions:\n",
    "        charter[\"date_info\"] = exceptions[charter_num]\n",
    "        title_paragraph = get_title_paragraph(charter)\n",
    "        charter[\"date_info\"][\"date_string\"] = \"\\n\".join(title_paragraph[\"lines\"])\n",
    "        date_found += 1\n",
    "    elif charter_num in missing_numbers:\n",
    "        charter[\"date_info\"] = make_unknown_date_info(date_string)\n",
    "        charter[\"date_info\"][\"date_string\"] = \"\"\n",
    "        date_not_found += 1\n",
    "        print(\"MISSING CHARTER:\", charter_num, date_string)\n",
    "    else:\n",
    "        charter[\"date_info\"] = parse_date_string(copy.copy(date_string), charter_num)\n",
    "        if charter[\"date_info\"]:\n",
    "            charter[\"date_info\"][\"date_string\"] = date_string\n",
    "            date_found += 1\n",
    "        else:\n",
    "            charter[\"date_info\"] = make_unknown_date_info(date_string)\n",
    "            date_not_found += 1\n",
    "    es.index(index=\"ohz\", doc_type=\"ohz-charter\", id=charter_num, body=charter)\n",
    "    charter_date[charter_num] = charter[\"date_info\"]\n",
    "    if charter_num % 100 == 0:\n",
    "        print(\"charter:\", charter_num, \"\\tdates found:\", date_found, \"\\tdates not found:\", date_not_found, \"\\tdates skipped:\", date_skipped)\n",
    "\n",
    "print(\"charter:\", charter_num, \"\\tdates found:\", date_found, \"\\tdates not found:\", date_not_found, \"\\tdates skipped:\", date_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the interpreted date information to an Excel file for manual data curation.\n",
    "\n",
    "import datetime\n",
    "\n",
    "headers = [\n",
    "    \"sort_year\", \"start_year\", \"end_year\", \n",
    "    \"sort_date\", \"start_date\", \"end_date\", \n",
    "    \"date_specificity\", \"date_certainty\", \"date_modifier\", \"date_string\"\n",
    "]\n",
    "\n",
    "\n",
    "from openpyxl import Workbook\n",
    "wb = Workbook()\n",
    "\n",
    "# grab the active worksheet\n",
    "ws = wb.active\n",
    "\n",
    "# Rows can also be appended\n",
    "ws.append([\"charter_number\"] + headers)\n",
    "\n",
    "\n",
    "for charter_num in charter_date:\n",
    "    for header in headers:\n",
    "        if header not in charter_date[charter_num]:\n",
    "            charter_date[charter_num][header] = None\n",
    "    row = [charter_num]\n",
    "    for header in headers:\n",
    "        value = charter_date[charter_num][header]\n",
    "        if isinstance(value, datetime.date):\n",
    "            value = value.strftime(\"%Y-%m-%d\")\n",
    "        row += [value]\n",
    "    ws.append(row)\n",
    "        \n",
    "    \n",
    "# Save the file\n",
    "wb.save(\"ohz-charter-dates.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Place Names based on the Charter Indices\n",
    "\n",
    "The charter indices mention place names with their spelling variants and identify the charter books, page numbers and line numbers on which these places are mentioned in the charters. This allows separating place names mentioned in charter text from place names mentioned in commentary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reference_query(reference):\n",
    "    return {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"match\": {\"book_number\": reference[\"book\"]}},\n",
    "                    {\"match\": {\"paragraphs.page_number\": reference[\"page\"]}},\n",
    "                    {\"match\": {\"paragraphs.line_numbers\": reference[\"line\"]}},\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 1000\n",
    "    }\n",
    "\n",
    "def get_paragraph_by_reference(reference, charter_data):\n",
    "    for paragraph in charter_data[\"paragraphs\"]:\n",
    "        if paragraph[\"book_number\"] != reference[\"book\"]:\n",
    "            continue\n",
    "        if paragraph[\"page_number\"] != reference[\"page\"]:\n",
    "            continue\n",
    "        if reference[\"line\"] not in paragraph[\"line_numbers\"]:\n",
    "            continue\n",
    "        return paragraph\n",
    "    return False\n",
    "\n",
    "def get_charter_by_reference(reference):\n",
    "    ref_query = make_reference_query(reference)\n",
    "    try:\n",
    "        response = es.search(index=\"ohz\", doc_type=\"ohz-charter\", body=ref_query)\n",
    "    except:\n",
    "        print(\"ERROR in querying charter by reference:\", reference)\n",
    "        print(ref_query)\n",
    "        raise\n",
    "    if response['hits']['total'] == 0:\n",
    "        # missing charter text\n",
    "        return []\n",
    "    return [hit['_source'] for hit in response['hits']['hits'] if get_paragraph_by_reference(reference, hit['_source'])]\n",
    "\n",
    "def get_neighbourhood(reference, charter):\n",
    "    paragraph = get_paragraph_by_reference(reference, charter)\n",
    "    try:\n",
    "        line_index = paragraph[\"line_numbers\"].index(reference[\"line\"])\n",
    "    except TypeError:\n",
    "        print(\"charter:\", charter)\n",
    "        print(\"paragraph:\", paragraph)\n",
    "        print(reference)\n",
    "        raise\n",
    "    neighbourhood = {\n",
    "        \"reference_line\": paragraph[\"lines\"][line_index]\n",
    "    }\n",
    "    ref_line = paragraph[\"lines\"][line_index]\n",
    "    if line_index > 0 and not paragraph[\"lines\"][line_index-1] == '':\n",
    "        neighbourhood[\"previous_line\"] = paragraph[\"lines\"][line_index-1]\n",
    "    elif line_index > 1 and paragraph[\"lines\"][line_index-1] == '':\n",
    "        neighbourhood[\"previous_line\"] = paragraph[\"lines\"][line_index-2]\n",
    "    else:\n",
    "        for paragraph in charter[\"paragraphs\"]:\n",
    "            if paragraph[\"page_number\"] == reference[\"page\"] and reference[\"line\"] - 1 in paragraph[\"line_numbers\"]:\n",
    "                line_index = paragraph[\"line_numbers\"].index(reference[\"line\"]-1)\n",
    "                neighbourhood[\"previous_line\"] = paragraph[\"lines\"][line_index]\n",
    "    if len(paragraph[\"lines\"]) > line_index+1 and not paragraph[\"lines\"][line_index+1] == '':\n",
    "        neighbourhood[\"next_line\"] = paragraph[\"lines\"][line_index+1]\n",
    "    elif len(paragraph[\"lines\"]) > line_index+2 and paragraph[\"lines\"][line_index+1] == '':\n",
    "        neighbourhood[\"next_line\"] = paragraph[\"lines\"][line_index+2]\n",
    "    else:\n",
    "        for paragraph in charter[\"paragraphs\"]:\n",
    "            if paragraph[\"page_number\"] == reference[\"page\"] and reference[\"line\"] + 1 in paragraph[\"line_numbers\"]:\n",
    "                line_index = paragraph[\"line_numbers\"].index(reference[\"line\"]+1)\n",
    "                neighbourhood[\"next_line\"] = paragraph[\"lines\"][line_index]\n",
    "    try:\n",
    "        if \"previous_line\" in neighbourhood and neighbourhood[\"previous_line\"][-1] == \"-\":\n",
    "            neighbourhood[\"previous_line_extended\"] = neighbourhood[\"previous_line\"][:-1] + neighbourhood[\"reference_line\"].strip()\n",
    "    except IndexError:\n",
    "        print(neighbourhood)\n",
    "        raise\n",
    "    if \"next_line\" in neighbourhood and neighbourhood[\"reference_line\"][-1] == \"-\":\n",
    "        neighbourhood[\"reference_line_extended\"] = neighbourhood[\"reference_line\"][:-1] + neighbourhood[\"next_line\"].strip()\n",
    "    return neighbourhood\n",
    "\n",
    "def map_book_name(book_name):\n",
    "    if book_name == \"I\" or book_name == \"I 1\":\n",
    "        return 1\n",
    "    if book_name == \"II\" or book_name == \"II 2\":\n",
    "        return 2\n",
    "    if book_name == \"III\" or book_name == \"III 3\":\n",
    "        return 3\n",
    "    if book_name == \"IV\" or book_name == \"IV 4\":\n",
    "        return 4\n",
    "    if book_name == \"V\" or book_name == \"V 5\":\n",
    "        return 5\n",
    "    raise ValueError(\"Invalid book name: {b}\".format(b=book_name))\n",
    "\n",
    "def make_empty_reference():\n",
    "    return {\n",
    "        \"book\": None,\n",
    "        \"page\": None,\n",
    "        \"line\": None,\n",
    "    }\n",
    "\n",
    "def parse_additional_index_text(item_soup, index_string):\n",
    "    for font_item in item_soup.find_all(\"font\"):\n",
    "        if len(font_item.attrs) > 0:\n",
    "            continue\n",
    "        index_string += font_item.text.strip(\"\\n\")\n",
    "        if index_string.count(\"(\") == index_string.count(\")\"):\n",
    "            return index_string\n",
    "    return index_string\n",
    "            \n",
    "\n",
    "def parse_index_name(item_soup, prev_name):\n",
    "    name_span_soup = item_soup.find(\"span\", class_=\"name\")\n",
    "    index_string = name_span_soup.text\n",
    "    name_pref = index_string.strip()\n",
    "    name_variants = []\n",
    "    if \" (\" in index_string:\n",
    "        if index_string.count(\"(\") > index_string.count(\")\"):\n",
    "            index_string = parse_additional_index_text(item_soup, index_string)\n",
    "        #print(\"parsing name:\")\n",
    "        match = re.search(r\" \\((.*)\\)\", index_string)\n",
    "        name_pref = re.sub(r\" \\((.*)\\).*\", \"\", index_string)\n",
    "        try:\n",
    "            name_variants = match.group(1).split(\", \")\n",
    "            for name_index, name_variant in enumerate(name_variants):\n",
    "                if \"(?)\" in name_variant:\n",
    "                    name_variant = name_variant.replace(\" (?)\",\"\")\n",
    "                    name_variants[name_index] = name_variant\n",
    "                    unsure_index_variant[name_pref] += [name_variant]\n",
    "        except AttributeError:\n",
    "            print(\"index_string:\\n\\t\", index_string)\n",
    "            raise\n",
    "        #print(reference)\n",
    "    elif index_string.startswith(\"{p}, \".format(p=prev_name)):\n",
    "        name_pref = prev_name\n",
    "    return name_pref, name_variants\n",
    "\n",
    "def parse_reference_span(item_soup, name_pref, unresolvable_references):\n",
    "    name_references = []\n",
    "    ref_span = item_soup.find(\"span\", class_=\"references\")\n",
    "    if not ref_span:\n",
    "        return name_references\n",
    "    reference = make_empty_reference()\n",
    "    for child in ref_span.children:\n",
    "        if child.name == \"b\":\n",
    "            try:\n",
    "                reference[\"book\"] = map_book_name(child.text.strip())\n",
    "            except:\n",
    "                #print(\"Adding entry_text:\")\n",
    "                #print(item_soup)\n",
    "                #reference[\"entry_text\"] = item_soup.text\n",
    "                #invalid_references[name_pref] += [reference]\n",
    "                continue\n",
    "        if child.name == \"a\":\n",
    "            reference[\"page\"] = int(child.text.strip())\n",
    "        if child.name == \"font\" and 'class' in child.attrs and \"pageLine\" in child.attrs['class']:\n",
    "            reference[\"line\"] = int(child.text.strip())\n",
    "            if not reference[\"book\"] or not reference[\"page\"]:\n",
    "                print(\"Missing reference data!\")\n",
    "                #print(name_pref)\n",
    "                #print(item_soup)\n",
    "                reference[\"entry_text\"] = item_soup.text.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "                reference[\"problem_type\"] = \"unparseable_index_entry\"\n",
    "                unresolvable_references[name_pref] += [reference]\n",
    "                continue\n",
    "            if \"entry_text\" in reference:\n",
    "                print(name_pref)\n",
    "                print(item_soup)\n",
    "                raise KeyError(\"This reference should not have an entry_text property\")\n",
    "            name_references += [reference]\n",
    "            reference = copy.copy(reference)\n",
    "            if \"entry_text\" in reference:\n",
    "                del reference[\"entry_text\"]\n",
    "    return name_references\n",
    "\n",
    "def lookup_reference_exact(name_pref, variants, reference, ref_line, charter):\n",
    "    for name_variant in [name_pref] + variants[name_pref]:\n",
    "        if name_variant not in ref_line:\n",
    "            continue\n",
    "        reference['place_string'] = name_variant\n",
    "        reference['place_pref'] = name_pref\n",
    "        reference[\"match_type\"] = \"exact_match\"\n",
    "        return True\n",
    "\n",
    "def lookup_reference_fuzzy(name_pref, variants, reference, ref_line, charter):\n",
    "    max_score = 0\n",
    "    best_match = None\n",
    "    best_score = None\n",
    "    for name_variant in [name_pref] + variants[name_pref]:\n",
    "        try:\n",
    "            term_matches = fuzzy_matcher.find_term_matches(ref_line, name_variant, max_length_variance=0)\n",
    "            term_matches += fuzzy_matcher.find_term_matches(ref_line, name_variant, max_length_variance=1)\n",
    "            term_matches += fuzzy_matcher.find_term_matches(ref_line, name_variant, max_length_variance=2)\n",
    "        except:\n",
    "            print(ref_line, name_variant)\n",
    "            raise\n",
    "        #print(\"Term matches:\", name_variant, term_matches)\n",
    "        candidate_scores = fuzzy_matcher.filter_candidates(term_matches, name_variant)\n",
    "        candidate_scores = fuzzy_matcher.rank_candidates(term_matches, name_variant)\n",
    "        #candidates = fuzzy_matcher.find_candidates(ref_line, name_variant)\n",
    "        for candidate_score in candidate_scores:\n",
    "            if candidate_score[\"total\"] > max_score:\n",
    "                max_score = candidate_score[\"total\"]\n",
    "                best_match = {\"name_variant\": name_variant, \"text_variant\": candidate_score[\"candidate\"], \"score\": max_score}\n",
    "                best_score = candidate_score\n",
    "    #print(\"BEST SCORE:\", best_score)\n",
    "    return best_match\n",
    "\n",
    "def lookup_reference(name_pref, variants, reference, charter):\n",
    "    order = [\"reference_line\", \"reference_line_extended\", \"previous_line\", \"previous_line_extended\", \"next_line\"]\n",
    "    paragraph = get_paragraph_by_reference(reference, charter)\n",
    "    neighbourhood = get_neighbourhood(reference, charter)\n",
    "    overall_best_match = None\n",
    "    overall_best_score = 0\n",
    "    overall_best_line = None\n",
    "    for line_type in order:\n",
    "        if line_type not in neighbourhood:\n",
    "            continue\n",
    "        if lookup_reference_exact(name_pref, variants, reference, neighbourhood[line_type], charter):\n",
    "            reference[\"match_line\"] = line_type\n",
    "            return True\n",
    "        best_match = lookup_reference_fuzzy(name_pref, variants, reference, neighbourhood[line_type], charter)\n",
    "        if best_match and best_match[\"score\"] > overall_best_score:\n",
    "            overall_best_score = best_match[\"score\"]\n",
    "            overall_best_match = best_match\n",
    "            overall_best_line = line_type\n",
    "    if overall_best_match and overall_best_match[\"score\"] > 5:\n",
    "        #print(\"Best match:\", overall_best_match)\n",
    "        reference['place_variant'] = overall_best_match[\"name_variant\"]\n",
    "        reference['place_string'] = overall_best_match[\"text_variant\"]\n",
    "        reference['place_pref'] = name_pref\n",
    "        reference[\"match_type\"] = \"best_match\"\n",
    "        reference[\"match_line\"] = overall_best_line\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_unresolvable_reference(name_pref, reference, charters):\n",
    "    if is_reference_to_missing_page(name_pref, reference):\n",
    "        return True\n",
    "    elif len(charters) == 0:\n",
    "        if is_addenda_reference(name_pref, reference):\n",
    "            return True\n",
    "        else:\n",
    "            is_other_error_reference(name_pref, reference)\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def is_addenda_reference(name_pref, reference):\n",
    "    if reference[\"page\"] > last_page[\"OHZ{b}\".format(b=reference[\"book\"])]:\n",
    "        reference[\"problem_type\"] = \"addenda_reference\"\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_reference_to_missing_page(name_pref, reference):\n",
    "    if reference[\"page\"] in missing_pages[reference[\"book\"]]:\n",
    "        reference[\"problem_type\"] = \"unavailable_page_reference\"\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_other_error_reference(name_pref, reference):\n",
    "    reference[\"problem_type\"] = \"unresolved_reference\"\n",
    "\n",
    "def line_has_reference(name_pref, variants, reference, charters):\n",
    "    reference[\"charter\"] = charters[0][\"charter_number\"]\n",
    "    if lookup_reference(name_pref, variants, reference, charters[0]):\n",
    "        references[name_pref] += [reference]\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def prev_line_has_reference(name_pref, variants, reference, charters):\n",
    "    prev_line_reference = copy.copy(reference)\n",
    "    prev_line_reference[\"line\"] -= 1\n",
    "    prev_line_charters = get_charter_by_reference(prev_line_reference)\n",
    "    if len(prev_line_charters) == 0 or prev_line_charters[0] in charters:\n",
    "        return False\n",
    "    prev_line_reference[\"charter\"] = prev_line_charters[0][\"charter_number\"]\n",
    "    return line_has_reference(name_pref, variants, prev_line_reference, prev_line_charters)\n",
    "    \n",
    "def next_line_has_reference(name_pref, variants, reference, charters):\n",
    "    next_line_reference = copy.copy(reference)\n",
    "    next_line_reference[\"line\"] += 1\n",
    "    next_line_charters = get_charter_by_reference(next_line_reference)\n",
    "    if len(next_line_charters) == 0 or next_line_charters[0] in charters:\n",
    "        return False\n",
    "    next_line_reference[\"charter\"] = next_line_charters[0][\"charter_number\"]\n",
    "    return line_has_reference(name_pref, variants, next_line_reference, next_line_charters)\n",
    "    \n",
    "def parse_index_item(item_soup, variants, prev_name, references, unresolvable_references):\n",
    "    name_pref, name_variants = parse_index_name(item_soup, prev_name)\n",
    "    if name_pref not in has_variants:\n",
    "        return None\n",
    "    if name_pref not in variants:\n",
    "        print(\"Resolving index entries for\", name_pref)\n",
    "    variants[name_pref] += name_variants\n",
    "    name_references = parse_reference_span(item_soup, name_pref, unresolvable_references)\n",
    "    for reference in name_references:\n",
    "        charters = get_charter_by_reference(reference)\n",
    "        if is_unresolvable_reference(name_pref, reference, charters):\n",
    "            unresolvable_references[name_pref] += [reference]\n",
    "            continue\n",
    "        if line_has_reference(name_pref, variants, reference, charters):\n",
    "            continue\n",
    "        if prev_line_has_reference(name_pref, variants, reference, charters):\n",
    "            continue\n",
    "        if next_line_has_reference(name_pref, variants, reference, charters):\n",
    "            continue\n",
    "        reference[\"problem_type\"] = \"unresolved_reference\"\n",
    "        unresolvable_references[name_pref] += [reference]\n",
    "    return name_pref\n",
    "    \n",
    "def char_range(c1, c2):\n",
    "    \"\"\"Generates the characters from `c1` to `c2`, inclusive.\"\"\"\n",
    "    for c in range(ord(c1), ord(c2)+1):\n",
    "        yield chr(c)\n",
    "        \n",
    "fuzzy_matcher = FuzzyMatcher(char_match_threshold=0.7, ngram_threshold=0.7, levenshtein_threshold=0.7)\n",
    "\n",
    "found = 0 # for references that have an exact or best match\n",
    "not_found = 0 # for references that cannot be found in the text (either word is not in OCR or reference is incorrect)\n",
    "missing_ref_text = 0 # for references to pages that are not in the hOCR output\n",
    "addenda_ref = 0 # for references to addenda pages at the end of charter books\n",
    "\n",
    "references = defaultdict(list)\n",
    "unresolvable_references = defaultdict(list)\n",
    "unsure_index_variant = defaultdict(list)\n",
    "variants = defaultdict(list)\n",
    "\n",
    "for index_char in char_range(\"A\", \"Z\"):\n",
    "    prev_name = None\n",
    "    index_file = \"ohz/ohz_index/ohz_index-{i}.html\".format(i=index_char)\n",
    "    with open(index_file, 'rt') as fh:\n",
    "        index_soup = bsoup(fh, \"lxml\")\n",
    "\n",
    "    items_soup = index_soup.findAll(\"div\", class_=\"item\")\n",
    "    for index, item_soup in enumerate(items_soup):\n",
    "        prev_name = parse_index_item(item_soup, variants, prev_name, references, unresolvable_references)\n",
    "        \n",
    "print(\"Index entries:\", len(references.keys()))\n",
    "print(\"Found:\", found, \"\\tNot found:\", not_found, \"\\tMissing refs:\", missing_ref_text, \"\\tAddenda refs:\", addenda_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"charter\", \"book\", \"page\", \"line\", \"place_pref\", \"place_variant\", \"place_string\", \"match_type\", \"match_line\"]\n",
    "\n",
    "from openpyxl import Workbook\n",
    "wb = Workbook()\n",
    "\n",
    "# grab the active worksheet\n",
    "ws = wb.active\n",
    "\n",
    "# Rows can also be appended\n",
    "ws.append(headers)\n",
    "\n",
    "for name_pref in references:\n",
    "    for reference in references[name_pref]:\n",
    "        row = []\n",
    "        for header in reference.keys():\n",
    "            if header not in headers:\n",
    "                print(\"missing header\", header)\n",
    "                print(reference)\n",
    "        if \"place_variant\" not in reference:\n",
    "            reference[\"place_variant\"] = reference[\"place_string\"]\n",
    "        for header in headers:\n",
    "            if header not in reference:\n",
    "                print(\"missing header\", header)\n",
    "                print(reference)\n",
    "            if header == \"charter\":\n",
    "                row += [\"-\".join([str(charter) for charter in reference[header]])]\n",
    "            else:\n",
    "                row += [reference[header]]\n",
    "        ws.append(row)\n",
    "    \n",
    "# Save the file\n",
    "wb.save(\"ohz-placename-resolved-references.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['problem_type', 'name_pref', 'book', 'page', 'line', 'charter', 'entry_text']\n",
    "\n",
    "from openpyxl import Workbook\n",
    "wb = Workbook()\n",
    "\n",
    "# grab the active worksheet\n",
    "ws = wb.active\n",
    "\n",
    "ws.append(headers)\n",
    "for name_pref in unresolvable_references:\n",
    "    for reference in unresolvable_references[name_pref]:\n",
    "        reference[\"name_pref\"] = name_pref\n",
    "        #reference[\"charter\"] = None\n",
    "        #reference[\"entry_text\"] = None\n",
    "        row = []\n",
    "        for header in headers:\n",
    "            if header not in reference:\n",
    "                reference[header] = None\n",
    "            if header == \"charter\" and isinstance(reference[header], list):\n",
    "                row += [\"-\".join([str(charter) for charter in reference[header]])]\n",
    "            elif header == \"entry_text\" and isinstance(reference[header], str):\n",
    "                row += [reference[header].replace(\"\\n\", \" \")]\n",
    "            else:\n",
    "                row += [reference[header]]\n",
    "        try:\n",
    "            ws.append(row)\n",
    "        except:\n",
    "            print(row)\n",
    "            print(reference)\n",
    "            raise\n",
    "\n",
    "# Save the file\n",
    "wb.save(\"ohz-placename-unresolved-references.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "\n",
    "The text of the charter books are a mix between original charter text and commentary. Place names mentioned in the commentaries should be ignored. One distinguishing feature between commentary and charter text is that commentary is always in modern Dutch, while charter text can be either middle Dutch, French or Latin. Another is that commentary contains many formulaic phrases, names of editors and numbers.\n",
    "\n",
    "The following step is an attempt to detect language per paragraph to identify whether a paragraph is commentary or original charter text.\n",
    "\n",
    "As the previous step of recognizing place name references from the existing index provides good results, we skip the language detection aspect for now. We can consider revisiting this later when we want to identify e.g. references to the [Digitale Charterbank](https://www.huygens.knaw.nl/digitale-charterbank-nederland/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "\n",
    "latin_text = \"noluerit, inprimitus $? iram Dei omnipotentis incurrat et sanctorum angelorum, et a [limi]nibus   ecclesiarum *? vel consortio christianorum efficiatur extraneus, et habeat partem cum Tuda qui Dominum tradidit ?? et cum Dathan et Abyron w? quos terra vivos deglutivit 2?, et insuper inferat ? una cum socio fisco auri libras X, argenti pondo <L)> +? coactus exsolvat +, et quod repetit evindicare non valeat. \"\n",
    "modern_dutch_text = \"Drukken (alle, indien gedateerd, ad 726, tenzij anders vermeld): a. C. Scribani, Origines Antverpiensium, Antwerpiae 1610, p. 59-62. ? b. Miraeus, Cod. donationum, p. 31-35, nr. 8, naar a. ? c. Miraeus, Notitia, p. 23-24, fragmenten naar b. ? d. W. Bosschaerts, AtotpiBou de primis veteris Frisiae apostolis, Mechliniae 1650, p. 493-498, naar ab. ? e. Vredius, Hist. comitum Flandriae, IT, p. 317-319, naar b. ? f. C. Le Cointe, Annales ecclesiastici Francorum, IV, Parisiis 1670, p. 743-744. ? 8. J. Le Roy, Notitia marchionatus sacri Romani imperii hoc est urbis et agri Antverpiensis, Amstelaedami 1678, p. 67-68, naar B.?h. J. B. Gramaye, Antiquitates Bredanae, p. 6-7, in: Antiquitates illustrissimi ducatus Brabantia, Lovanii-Bruxellis 1708, onvolledig. ? i. Van Heussen, Batavia sacra, p. 40-41. ? j. Miraeus-Foppens, Opera dipl., T, p. rr, codex nr. 8, naar b. ? k. Van Loon, Hollandsche histori, p. 325-326, in noot, naar b. ? L. J. Bertholet, Histoire eccl?stastique et civile du duch? de Luxembourg et comt? de Chiny, IL, Luxembourg 1742, pi?ces justificatives p. 33-34. ? m. Van Goor, Beschrijving Breda, p. 400402, nr. 2. ? n. Dom Calmet, Histoire eccl?siastique et civile de la Lorraine, IL, Nancy 1748, preuves kol. xcii-xciv. ? o. Hontheim, Hist. Trevirensis, T, p. 115116, nr. 41, naar j. ? p. J.C. Diercxsens, Antverpia Christo nascens et crescens, I, Antverpiae 1773, p. 39-40, naar g. ? q. Gallia Christiana, XTII, instrumenta kol. 296-297, instr. eccl. Trevirensis nr. 10. ? r. L. G. O. de Br?quignyen F. J. G. La Porte du Theil, Diplomata, chartae, epistolae et alia documenta ad res Francicas spectantia, pars prima, I, Parisiis 179, p. 451-452. ?   J. M. Pardessus en L. G. O. de Br?quigny, Diplomata, chartae, epistolae, leges aliaque instrumenta ad res Gallo-Francicas spectantia, II, Parisiis 1649, p. 349-350, nr. 540, naar T. ? t. Migne, Patrologia Latina, LXXXIX, kol. 554-556, dipl. ad S. Willibrordum vel ab eo collata nr. 17. ? u. Van den Bergh, OHZ, L, p. 2, nr. 3, naar j en een afschrift naar B. ? v. F.-X.  Wurth-Paquet,  Table analytique des chartes et documents concernant la ville d Echternach et ses ?tablissements, I, Luxembourg \"\n",
    "modern_dutch_text = \"De tekstoverlevering van deze oorkonde is niet bijzonder goed. Poncelet (a.w., p. 166) neemt aan, dat de drukken alle direct of indirect zijn afgeleid van het oudst bekende afschrift B. Zeer waarschijnlijk staat ook geen der overige afschriften los van B. Enkele corrupte plaatsen in B zijn reeds door anderen ge?mendeerd. Het ge?mendeerde plaatsen wij tussen rechte haken. Dit staat los van de vraag naar de echtheid van deze oorkonde. Zij ts voor het eerst gesteld door J. Mabillon (Acta Sanctorum ordinis sancti Benedicti, saec. IIL-t, Lut. Paris. 1672, p. 629). Mabillon zelf heeft haar onbeantwoord gelaten. Geen belang heeft thans nog de kritiek op deze oorkonde uitgebracht door P. P. M. Alberdingk Thijm (De H. Willibrordus apostel der Nederlanden, Amsterdam-Leuven 1661, p. 255-257; vgl. de vertaling door L. Tross, Der heilige Willibrord, M?nster[W. 1863, p. 180, noot 2). Later is deze kritiek uitgewerkt door L. van der Essen in: Geschiedkundige Bladen, I-2, Amsterdam 1905, p. 378-382, waar Diederik van Echternach wordt voorgesteld als de vervaardiger circa 11Q0 van deze oorkonde. Kort nadien evenwel heeft Van der Essen deze hypothese weer ingetrokken, daartoe genoopt door de kritiek die hij had ontmoet bij A. Poncelet (a.w., p. 163-174; vgl. W. Levison, in: MGH, SS rer. Merov., VILT, p. gr, noot 9). Poncelet resumeerde zijn oordeel over de oorkonde, het testamentum van Willibrord, aldus:  Tout compte fait, s?il n'est peut-?tre pas sage de se prononcer r?solument pour Dauthenticit? du ?testament de S. Willibrord, il semble infiniment moins prudent encore de le ranger parmi les documents apocryphes.                                  | Er is geen reden om er aan te twijfelen dat Willibrord in het zesde jaar van koning Diederik TV een aantal bij missionering verworven bezittingen met een oorkonde zou hebben geschonken aan het klooster te Echternach, en dat de tekst van deze oorkonde bewaard is gebleven. Dit neemt niet weg, dat deze tekst in de enige ons bewaard gebleven versie bij nadere beschouwing een gewichtige interpolatie bevat, namelijk de passage waaruit moet blijken dat Willibrord, behalve bezittingen afkomstig van een aantal ingenui Franci, aan Echternach ook goederen heeft geschonken, die hij had ontvangen van de familie der Pippiniden. Wat Wampach (a.w., Le, p. go vig. ) hierover ook moge gezegd hebben, het blijft een feit dat deze passage op onhandige wijze een contekst verstoort, die bovendien verderop met de inhoud van deze toch belangrijke inlas weer geen rekening houdt. Tot de onechte passages moet verder het zinsdeel vel ad illam sanctam congre\"\n",
    "\n",
    "print(detect(latin_text))\n",
    "print(detect_langs(latin_text))\n",
    "\n",
    "terms = re.split(r\"\\W+\", latin_text.lower())\n",
    "terms = re.split(r\"\\W+\", modern_dutch_text.lower())\n",
    "\n",
    "# Modern Dutch text characteristics:\n",
    "# - many acronyms /[a-zA-Z]\\./ (single character followed by dot)\n",
    "# - many numbers\n",
    "# - many page references: /p\\. \\d+/\n",
    "# - many domain-specific phrases: [\"Drukken\", \"tenzij anders vermeld\", \"afschrift\", \"waarschijnlijk\", \"oorkonde\"]\n",
    "# - many typical names of editors, scholars: [\"Poncelet\", \"Dijkhof\", \"Wampach\"]\n",
    "\n",
    "# Latin text characteristics:\n",
    "# - many typical suffixes: /.*(um|ur|us|is)/\n",
    "# - few numbers\n",
    "\n",
    "# Middle Dutch text characteristics:\n",
    "# - many typical ngrams: /(ae|ugh|ck|)\n",
    "# - few numbers\n",
    "\n",
    "# French text characteristics\n",
    "\n",
    "tf = Counter(terms)\n",
    "tf.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
